{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> R snippet: </h3>\n",
    "\n",
    "- remove duplicates based on lat,long,species,year \n",
    "- add seasons info\n",
    "\n",
    "Use R kernel to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "\n",
    "total = read.csv(\"GBif_Original.csv\", stringsAsFactors = FALSE, sep=\"\\t\")\n",
    "\n",
    "unq_rows = as.numeric(rownames(unique(total[c(\"species\",\"year\",\"decimalLatitude\",\"decimalLongitude\")])))\n",
    "total_unq = total[unq_rows,]\n",
    "\n",
    "total$coor = paste0(total$decimalLatitude, total$decimalLongitude)\n",
    "total_unq$coor = paste0(total_unq$decimalLatitude, total_unq$decimalLongitude)\n",
    "month_count = total %>% group_by(species, year, coor) %>% summarise(paste(unique(month), collapse = \", \"))\n",
    "colnames(month_count)[ncol(month_count)] = \"months\"\n",
    "\n",
    "total_unq = total_unq[order(total_unq$species, total_unq$year, total_unq$coor),]\n",
    "month_count = month_count[order(month_count$species, month_count$year, month_count$coor),]\n",
    "#total_unq$months = month_count$`paste(unique(month), collapse = \", \")`\n",
    "#sep_months = sapply(total_unq$months, strsplit, \", \")\n",
    "sep_months = month_count$months\n",
    "\n",
    "total_unq$Winter = 0\n",
    "total_unq$Spring = 0\n",
    "total_unq$Summer = 0\n",
    "total_unq$Fall = 0\n",
    "for (row in 1:nrow(total_unq)) {\n",
    "  if (any(c(1,2,12) %in% sep_months[row][[1]][1])) {total_unq$Winter[row] = 1}\n",
    "  if (any(c(3:5) %in% sep_months[row][[1]][1])) {total_unq$Spring[row] = 1}\n",
    "  if (any(c(6:8) %in% sep_months[row][[1]][1])) {total_unq$Summer[row] = 1}\n",
    "  if (any(c(9:11) %in% sep_months[row][[1]][1])) {total_unq$Fall[row] = 1}\n",
    "  #if (row %% 15000 == 0) {print(paste(\"Loop is \", round(row/nrow(total_unq)*100), \"% done\", sep = \"\"))}\n",
    "}\n",
    "\n",
    "total_unq$coor = NULL\n",
    "\n",
    "write.csv(total_unq, \"GBif_R.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Python snippets: </h3>\n",
    "\n",
    "- convert dataset keys to dataset names\n",
    "- add redList designation\n",
    "- add common names from ITIS\n",
    "- drop unnecessary columns\n",
    "- add geometry from lat, long\n",
    "- convert data to species per year in geojson\n",
    "\n",
    "Use Python2 kernel to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "# from geojson import GeometryCollection\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from python_helpers import column_helpers as ch\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"GBif_R.csv\", low_memory=False)\n",
    "df = applyTransformations(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def yearSpeciesCount(df):\n",
    "    df = df.dropna(subset = ['year'])\n",
    "    df.year = df['year'].astype(int)\n",
    "    gp_series = df.groupby(['year']).size()\n",
    "    year_counts = list(zip(gp_series.index, gp_series))\n",
    "\n",
    "def some(df, n):\n",
    "    return df.loc[random.sample(df.index, n)]\n",
    "    \n",
    "def testingFilter(df):\n",
    "    df = df[(df['year'] == 2017) | (df['year'] == 2016) | (df['year'] == 2015)]\n",
    "    df = df[(df[\"species\"] == \"Corvus caurinus\") | (df[\"species\"] == \"Turdus migratorius\") |\n",
    "           (df[\"species\"] == \"Larus glaucescens\")]\n",
    "    return df\n",
    "\n",
    "def applyTransformations(df):\n",
    "    df = ch.dataSetNamesFromKey(df, read_pickle=\"datasetKeyNames.pickle\")\n",
    "    df = ch.addCommonNames(df, \"ITIS.sqlite\")\n",
    "    df = ch.addRedList(df, \"redlist_assessments.csv\")\n",
    "    return df\n",
    "\n",
    "def pointCSVtoJSONPerYear(df):\n",
    "    df = df[['species', 'Winter', 'Spring', 'Summer', 'Fall', 'datasetName', 'common', \n",
    "                     'redList', 'decimalLatitude', 'decimalLongitude', 'year']]\n",
    "    geo_df = ch.toGEOdf(df)    \n",
    "    geo_df = geo_df.drop(['decimalLatitude', 'decimalLongitude'], 1)\n",
    "    gb = geo_df.groupby(['year'])\n",
    "    for k, gp in gb:\n",
    "#        gp.drop(['year'], 1)\n",
    "       gp.to_file(\"leaflet/gbif/\" + str(int(k)) + \".geojson\", driver=\"GeoJSON\")\n",
    "    return geo_df\n",
    "\n",
    "def pointCSVtoJSONPerYearPerSpecies(df):\n",
    "    df = df[['species', 'Winter', 'Spring', 'Summer', 'Fall', 'datasetName', 'common', \n",
    "                     'redList', 'decimalLatitude', 'decimalLongitude', 'year']]\n",
    "    geo_df = ch.toGEOdf(df)\n",
    "    geo_df = geo_df.drop(['decimalLatitude', 'decimalLongitude'], 1)\n",
    "    #split into species per year as gejsons            \n",
    "    gb_year = geo_df.groupby(['year'])\n",
    "    for k_year, gp_year in gb_year:\n",
    "        gp_year = gp_year.drop(['year'], 1)\n",
    "        gp_species = gp_year.groupby(['species'])\n",
    "        os.mkdir('leaflet/gbif_year_species/' + str(int(k_year)))\n",
    "        for k_species, gp_species in gp_species:\n",
    "            gp_species.to_file(\"map_django/biodivmap/static/biodivmap/gbif_year_species\" + str(int(k_year))\n",
    "                               + '/'+ k_species + \".geojson\", driver=\"GeoJSON\")\n",
    "        print(str(int(k_year)) + \" done!\")\n",
    "    return geo_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = 'map_django/biodivmap/static/biodivmap/'\n",
    "taxonLevels = ['gbif','kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species']\n",
    "file_count = 0\n",
    "\n",
    "def pointCSVtoJSONPerYearPerTaxonLimited(df, lim=1000):\n",
    "    df = df.dropna(axis=0, subset=['species'])\n",
    "    geo_df = ch.toGEOdf(df)\n",
    "    geo_df = geo_df.drop(['decimalLatitude', 'decimalLongitude'], 1)\n",
    "    #split into species per year as gejsons            \n",
    "    gb_year = geo_df.groupby(['year'])\n",
    "    os.mkdir(BASE_PATH + 'gbif/')\n",
    "    master_count = 0\n",
    "    dbCon = sqlite3.connect(\"map_django/db.sqlite3\")\n",
    "    cur   = dbCon.cursor()\n",
    "    for k_year, gp_year in gb_year:\n",
    "        gp_year = gp_year.drop(['year'], 1)\n",
    "        year_path = BASE_PATH + 'gbif/' + str(int(k_year)) + \"/\"\n",
    "        os.mkdir(year_path)\n",
    "        master_count += perTaxonWriterYear(0, gp_year, year_path, lim, cur, str(int(k_year)))\n",
    "        print(str(int(k_year)) + \" done!\")\n",
    "    dbCon.commit()\n",
    "    dbCon.close()\n",
    "    return geo_df, master_count\n",
    "\n",
    "def perTaxonWriterYear(taxonIndex, df, path, lim, cur, year):\n",
    "    if (taxonLevels[taxonIndex] == \"species\" or df.shape[0] <= lim):\n",
    "        for spec in list(df.species.unique()):\n",
    "            cur.execute(\"UPDATE species_year SET (%s) = ?  WHERE species = ?\" %(\"year_\"+ year),(path.replace(BASE_PATH,\"\"), spec,))\n",
    "        df.to_file(path + \"points\"+ \".geojson\", driver=\"GeoJSON\")\n",
    "        return 1\n",
    "    else:\n",
    "        gb_nextTaxonLevel = df.groupby([taxonLevels[taxonIndex+1]])\n",
    "        inner_count = 0\n",
    "        for k_nextTaxon, gp_nextTaxon in gb_nextTaxonLevel:\n",
    "            next_path = path + k_nextTaxon + \"/\"\n",
    "            os.mkdir(next_path)\n",
    "            inner_count+=perTaxonWriterYear(taxonIndex+1, gp_nextTaxon, next_path, lim, cur,year)\n",
    "        return inner_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = 'map_django/biodivmap/static/biodivmap/'\n",
    "taxonLevels = ['gbif', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species']\n",
    "file_count = 0\n",
    "\n",
    "def pointCSVtoJSONPerRecencyPerTaxonLimited(df, lim=1000):\n",
    "    df = df.dropna(axis=0, subset=['species'])\n",
    "    geo_df = ch.toGEOdf(df)\n",
    "    geo_df = geo_df.drop(['decimalLatitude', 'decimalLongitude'], 1)\n",
    "    # split into species per recency as gejsons\n",
    "    gb_rec = geo_df.groupby(['recency'])\n",
    "    os.mkdir(BASE_PATH + 'gbif/')\n",
    "    master_count = 0\n",
    "    dbCon = sqlite3.connect(\"map_django/db.sqlite3\")\n",
    "    cur = dbCon.cursor()\n",
    "    for k_rec, gp_rec in gb_rec:\n",
    "        # gp_rec = gp_rec.drop(['recency'], 1)\n",
    "        rec_path = BASE_PATH + 'gbif/' + k_rec + \"/\"\n",
    "        os.mkdir(rec_path)\n",
    "        master_count += perTaxonWriterRecency(0, gp_rec, rec_path, lim, cur, k_rec)\n",
    "        print(k_rec + \" done!\")\n",
    "    dbCon.commit()\n",
    "    dbCon.close()\n",
    "    return geo_df, master_count\n",
    "\n",
    "\n",
    "def perTaxonWriterRecency(taxonIndex, df, path, lim, cur, rec):\n",
    "    if (taxonLevels[taxonIndex] == \"species\" or df.shape[0] <= lim):\n",
    "        for spec in list(df.species.unique()):\n",
    "            cur.execute(\"UPDATE biodivmap_speciesrecency SET (%s) = ?  WHERE species = ?\" % (rec),\n",
    "                        (path.replace(BASE_PATH, \"\"), spec,))\n",
    "        df.to_file(path + \"points\" + \".geojson\", driver=\"GeoJSON\")\n",
    "        return 1\n",
    "    else:\n",
    "        gb_nextTaxonLevel = df.groupby([taxonLevels[taxonIndex + 1]])\n",
    "        inner_count = 0\n",
    "        for k_nextTaxon, gp_nextTaxon in gb_nextTaxonLevel:\n",
    "            next_path = path + k_nextTaxon + \"/\"\n",
    "            os.mkdir(next_path)\n",
    "            inner_count += perTaxonWriterRecency(taxonIndex + 1, gp_nextTaxon, next_path, lim, cur, rec)\n",
    "        return inner_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.read_csv(\"GBif June27.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 0'], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['recent', 'old', 'both'], dtype=object)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.recency.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'toGEOdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-70-2864afa52422>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpointCSVtoJSONPerRecencyPerTaxonLimited\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-62-012534999f57>\u001b[0m in \u001b[0;36mpointCSVtoJSONPerRecencyPerTaxonLimited\u001b[0;34m(df, lim)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpointCSVtoJSONPerRecencyPerTaxonLimited\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'species'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mgeo_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoGEOdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mgeo_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeo_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'decimalLatitude'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'decimalLongitude'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# split into species per recency as gejsons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'toGEOdf'"
     ]
    }
   ],
   "source": [
    "pointCSVtoJSONPerRecencyPerTaxonLimited(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeYearsColsDB(years, db_path):\n",
    "    col_query = \"\"\"CREATE TABLE species_year (\n",
    "    species TEXT NOT NULL PRIMARY KEY,\"\"\"\n",
    "    for year in years:\n",
    "        if (year == 2019):\n",
    "            col_query+=\"\"\"year_\"\"\" + str(year) + \"\"\" INTEGER DEFAULT 0\"\"\"\n",
    "        else:\n",
    "            col_query+=\"\"\"year_\"\"\" + str(year) + \"\"\" INTEGER DEFAULT 0,\"\"\"\n",
    "    col_query += \"\"\");\"\"\" \n",
    "    \n",
    "    dbCon = sqlite3.connect(db_path)\n",
    "    cur   = dbCon.cursor()\n",
    "    cur.execute(col_query)\n",
    "    dbCon.commit()\n",
    "    dbCon.close()\n",
    "\n",
    "\n",
    "def fillSpeciesIntoDB(df, db_path):\n",
    "    dbCon = sqlite3.connect(db_path)\n",
    "    cur   = dbCon.cursor()\n",
    "    for spec in list(df.species.unique()):\n",
    "        cur.execute(\"INSERT INTO biodivmap_speciesrecency (species) VALUES (?)\",(spec,))\n",
    "    dbCon.commit()\n",
    "    dbCon.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "fillSpeciesIntoDB(df, \"map_django/db.sqlite3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "for key,gp in geo_df.groupby([\"year\"]):\n",
    "    sum+=gp.species.unique().shape[0]\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"GBif_recency.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop([\"Unnamed: 0\"],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rec = df[df['recency'] == 'recent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> SHAPE FILES </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHP data to geoJSON\n",
    "def fix_crs(map_ob):\n",
    "    return map_ob.to_crs({'init': 'epsg:4326'})\n",
    "# str_map = gpd.read_file(\"ecological_reserves/BC_Eco_Reserves.shp\")\n",
    "str_map = gpd.read_file(\"MVSEI2014/MVSEI2014.shp\")\n",
    "\n",
    "str_map = fix_crs(str_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GeoJSON does not support multipolygon. Doesn't work\n",
    "# str_map.to_file(\"leaflet/UBC_poly.geojson\", driver=\"GeoJSON\")\n",
    "\n",
    "# # fiona doesn't work\n",
    "# import fiona\n",
    "# import json\n",
    "\n",
    "# with fiona.open('ecological_reserves/BC_Eco_Reserves.shp') as source:\n",
    "#     records = list(source)\n",
    "# geo_json = {\"type\": \"FeatureCollection\",\"features\": records}\n",
    "# with open('leaflet/UBC_poly.geojson', 'w') as fp:\n",
    "#     json.dump(geo_json, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert multipolygon to single polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom_series = str_map.geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geom_apply(x):\n",
    "    try:\n",
    "        return list(x)\n",
    "    except TypeError:\n",
    "        return [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom_series = geom_series.apply(geom_apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom_series = geom_series.apply(pd.Series).stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quick but loses properties\n",
    "# list_poly = list(geom_series)\n",
    "# geoms = GeometryCollection(list_poly)\n",
    "# geo_file = geojson.dumps(geoms)\n",
    "# with open(\"leaflet/UBC_poly.geojson\", \"w\") as text_file:\n",
    "#     text_file.write(geo_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Careful! Deep copy required here to avoid chaining\n",
    "\n",
    "df = pd.DataFrame(columns=str_map.columns)\n",
    "for ind, poly in geom_series.iteritems():\n",
    "    curr_row = str_map.loc[ind[0]].copy(deep=True)\n",
    "    curr_row['geometry'] = poly\n",
    "    df = df.append(curr_row)\n",
    "\n",
    "df_gpd = gpd.GeoDataFrame(df,geometry = df.geometry, crs = {'init': 'epsg:4326'})\n",
    "df_gpd.to_file(\"leaflet/SEI.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHP data to geoJSON\n",
    "def fix_crs(map_ob):\n",
    "    return map_ob.to_crs({'init': 'epsg:4326'})\n",
    "# str_map = gpd.read_file(\"ecological_reserves/BC_Eco_Reserves.shp\")\n",
    "str_map = gpd.read_file(\"MVSEI2014/MVSEI2014.shp\")\n",
    "\n",
    "str_map = fix_crs(str_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_map.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_map.Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_map = str_map[['SEI_PolyNb', 'Comp1Lgnd', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_map.to_file(\"leaflet/SEI.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_gdf_geometry(gdf, geom_type):\n",
    "    geometry = gdf.geometry\n",
    "    flattened_geometry = []\n",
    "\n",
    "    flattened_gdf = gpd.GeoDataFrame()\n",
    "\n",
    "    for geom in geometry:\n",
    "        if geom.type in ['GeometryCollection', 'MultiPoint', 'MultiLineString', 'MultiPolygon']:\n",
    "            for subgeom in geom:\n",
    "                if subgeom.type==geom_type:\n",
    "                    flattened_geometry.append(subgeom)\n",
    "        else:\n",
    "            if geom.type==geom_type:\n",
    "                flattened_geometry.append(geom)\n",
    "\n",
    "    flattened_gdf.geometry=flattened_geometry\n",
    "\n",
    "    return flattened_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = flatten_gdf_geometry(str_map, 'Polygon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"GBif_R.csv\", low_memory=False)\n",
    "#     #drop nan species\n",
    "# df = df.dropna(axis=0, subset=['species'])\n",
    "# #keep num_rows if num_rows > 0\n",
    "# #convert dataset keys to dataset names    \n",
    "# df = dataSetNamesFromKey(df)\n",
    "# print(\"Converted dataset keys to names!\")\n",
    "# #add redList designation\n",
    "# df['redList'] = df.apply(lambda x: redList(x), 1).values\n",
    "# print(\"Added redlist designation!\")\n",
    "# #add common names from ITS\n",
    "# df = addCommonNames(df)\n",
    "# print(\"Added common names!\")\n",
    "# df = df[['Winter', 'Spring', 'Summer', 'Fall', 'datasetName', 'common', \n",
    "#                  'redList', 'decimalLatitude', 'decimalLongitude', 'year',\n",
    "#         'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species']]\n",
    "\n",
    "# df.to_csv(\"Gbif_Gabe.csv\", encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupy2)",
   "language": "python",
   "name": "jupy2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
