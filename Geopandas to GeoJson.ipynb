{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> R snippet: </h3>\n",
    "\n",
    "- remove duplicates based on lat,long,species,year \n",
    "- add seasons info\n",
    "\n",
    "Use R kernel to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(tidyverse)\n",
    "\n",
    "total = read.csv(\"GBif_Original.csv\", stringsAsFactors = FALSE, sep=\"\\t\")\n",
    "\n",
    "unq_rows = as.numeric(rownames(unique(total[c(\"species\",\"year\",\"decimalLatitude\",\"decimalLongitude\")])))\n",
    "total_unq = total[unq_rows,]\n",
    "\n",
    "total$coor = paste0(total$decimalLatitude, total$decimalLongitude)\n",
    "total_unq$coor = paste0(total_unq$decimalLatitude, total_unq$decimalLongitude)\n",
    "month_count = total %>% group_by(species, year, coor) %>% summarise(paste(unique(month), collapse = \", \"))\n",
    "colnames(month_count)[ncol(month_count)] = \"months\"\n",
    "\n",
    "total_unq = total_unq[order(total_unq$species, total_unq$year, total_unq$coor),]\n",
    "month_count = month_count[order(month_count$species, month_count$year, month_count$coor),]\n",
    "#total_unq$months = month_count$`paste(unique(month), collapse = \", \")`\n",
    "#sep_months = sapply(total_unq$months, strsplit, \", \")\n",
    "sep_months = month_count$months\n",
    "\n",
    "total_unq$Winter = 0\n",
    "total_unq$Spring = 0\n",
    "total_unq$Summer = 0\n",
    "total_unq$Fall = 0\n",
    "for (row in 1:nrow(total_unq)) {\n",
    "  if (any(c(1,2,12) %in% sep_months[row][[1]][1])) {total_unq$Winter[row] = 1}\n",
    "  if (any(c(3:5) %in% sep_months[row][[1]][1])) {total_unq$Spring[row] = 1}\n",
    "  if (any(c(6:8) %in% sep_months[row][[1]][1])) {total_unq$Summer[row] = 1}\n",
    "  if (any(c(9:11) %in% sep_months[row][[1]][1])) {total_unq$Fall[row] = 1}\n",
    "  #if (row %% 15000 == 0) {print(paste(\"Loop is \", round(row/nrow(total_unq)*100), \"% done\", sep = \"\"))}\n",
    "}\n",
    "\n",
    "total_unq$coor = NULL\n",
    "\n",
    "write.csv(total_unq, \"GBif_R.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Python snippets: </h3>\n",
    "\n",
    "- convert dataset keys to dataset names\n",
    "- add redList designation\n",
    "- add common names from ITIS\n",
    "- drop unnecessary columns\n",
    "- add geometry from lat, long\n",
    "- convert data to species per year in geojson\n",
    "\n",
    "Use Python2 kernel to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "# from geojson import GeometryCollection\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Point data to geoJSON\n",
    "import random\n",
    "import sqlite3\n",
    "import os\n",
    "\n",
    "df_red = pd.read_csv(\"redlist_assessments.csv\")\n",
    "red_sciNames = df_red.scientificName\n",
    "\n",
    "def some(x, n):\n",
    "    return x.loc[random.sample(x.index, n)]\n",
    "\n",
    "def dataSetNamesFromKey(df):\n",
    "    k = df['datasetKey'].unique()\n",
    "#     n = ([])\n",
    "#     for key in k:\n",
    "#         url = \"http://api.gbif.org/v1/dataset/\" + key\n",
    "#         name = requests.get(url).json()['title']\n",
    "#         n.append(name)\n",
    "#     conv = dict(zip(k,n))  \n",
    "    \n",
    "#     pickle_out = open(\"dataSources.pickle\",'wb') \n",
    "#     pickle.dump(conv, pickle_out)   \n",
    "#     pickle_out.close()\n",
    "    conv = pd.read_pickle(\"datasetKeyNames.pickle\")\n",
    "    \n",
    "    df['datasetName'] = df['datasetKey']\n",
    "    df = df.replace({'datasetName': conv})\n",
    "    df = df.drop(columns='datasetKey')\n",
    "    return df\n",
    "\n",
    "# def observationsCountPerYear(df):\n",
    "#     gb = df.groupby(['year'])\n",
    "#     geo_df = df.dropna(subset = ['year'])\n",
    "#     geo_df['year'].astype(int)\n",
    "\n",
    "      # gbl2 missing\n",
    "#     year_dict = gbl2.to_dict()\n",
    "#     js = json.dumps(year_dict)\n",
    "    \n",
    "#     file = open('years_json.txt', 'w')\n",
    "#     json.dump(year_dict, file)\n",
    "#     file.close()\n",
    "\n",
    "def addCommonNames(geo_df):\n",
    "    \n",
    "    # df_commons['common'] = df_commons[['language', 'vernacular_name']].apply(lambda x: ':'.join(x), axis=1)\n",
    "    # df_commons = df_commons.groupby(['tsn'])['common'].apply(', '.join).reset_index()\n",
    "    db = sqlite3.connect('ITIS.sqlite')\n",
    "    df_commons = pd.read_sql_query(\"SELECT * from vernaculars\", db)\n",
    "    df_species = pd.read_sql_query(\"SELECT * from longnames\", db)\n",
    "\n",
    "    df_commons = df_commons[df_commons['language'] == \"English\"]\n",
    "    df_commons = df_commons.rename({'vernacular_name': 'common'}, axis=1)\n",
    "    df_commons = df_commons[['tsn', 'common']]\n",
    "    \n",
    "    df_species = df_species[df_species['completename'].isin(geo_df.species)]\n",
    "    df_commons = df_commons[df_commons['tsn'].isin(df_species.tsn)]\n",
    "    \n",
    "    df_commons = df_commons.groupby(['tsn'])['common'].apply(', '.join).reset_index()\n",
    "    df_species = df_species[df_species['tsn'].isin(df_commons.tsn)]\n",
    "\n",
    "    df_common_species = df_species.merge(df_commons, on=\"tsn\")\n",
    "    df_common_species = df_common_species.rename({'completename': 'species'}, axis=1) \n",
    "    df_common_species = df_common_species.drop(['tsn'], 1)\n",
    "\n",
    "    species = set(geo_df.species.unique())\n",
    "    species_in_itis = set(df_common_species.species.unique())\n",
    "    diff = list(species - species_in_itis)\n",
    "    diff = {df_common_species.columns[0]: diff, df_common_species.columns[1]: 'Common Name unknown'}\n",
    "    df = pd.DataFrame.from_dict(diff)\n",
    "    df_species = pd.concat([df_common_species, df], sort = False)\n",
    "\n",
    "    geo_df = geo_df.merge(df_species, on=\"species\")\n",
    "    return geo_df\n",
    "\n",
    "def redList(x):\n",
    "    if (x['species'] in red_sciNames.values):\n",
    "        return df_red.loc[red_sciNames[red_sciNames == x['species']].index[0]].redlistCategory\n",
    "    else:\n",
    "        return float('nan')\n",
    "    \n",
    "    \n",
    "def testingFilter(df):\n",
    "    df = df[(df['year'] == 2017) | (df['year'] == 2016) | (df['year'] == 2015)]\n",
    "    df = df[(df[\"species\"] == \"Corvus caurinus\") | (df[\"species\"] == \"Turdus migratorius\") |\n",
    "           (df[\"species\"] == \"Larus glaucescens\")]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointCSVtoJSONComplete(filename, num_rows=0):\n",
    "    ##open dataframe from R script that removes duplicates based on lat, long, year & species\n",
    "    #and adds seasons\n",
    "    df = pd.read_csv(filename, low_memory=False)\n",
    "    #drop nan species\n",
    "    df = df.dropna(axis=0, subset=['species'])\n",
    "    #keep num_rows if num_rows > 0\n",
    "    if num_rows:\n",
    "        df = some(df, num_rows)\n",
    "    #convert dataset keys to dataset names    \n",
    "    df = dataSetNamesFromKey(df)\n",
    "    print(\"Converted dataset keys to names!\")\n",
    "    #add redList designation\n",
    "    df['redList'] = df.apply(lambda x: redList(x), 1).values\n",
    "    print(\"Added redlist designation!\")\n",
    "    #add common names from ITS\n",
    "    df = addCommonNames(df)\n",
    "    print(\"Added common names!\")\n",
    "    df = df[['species', 'Winter', 'Spring', 'Summer', 'Fall', 'datasetName', 'common', \n",
    "                     'redList', 'decimalLatitude', 'decimalLongitude', 'year']]\n",
    "    #add geometry from lat, long\n",
    "#     geometry = [Point(xy) for xy in zip (df['decimalLongitude'], df['decimalLatitude'])]\n",
    "#     #fix coordinate system\n",
    "#     geo_df = gpd.GeoDataFrame(df, geometry = geometry, crs = {'init': 'epsg:4326'})\n",
    "#     geo_df = geo_df.drop(['decimalLatitude', 'decimalLongitude'], 1)\n",
    "    #split into species per year as gejsons            \n",
    "#     gb_year = geo_df.groupby(['year'])\n",
    "#     for k_year, gp_year in gb_year:\n",
    "#         gp_year = gp_year.drop(['year'], 1)\n",
    "#         gp_species = gp_year.groupby(['species'])\n",
    "#         os.mkdir('leaflet/gbif_year_species/' + str(int(k_year)))\n",
    "#         for k_species, gp_species in gp_species:\n",
    "#             gp_species.to_file(\"map_django/biodivmap/static/biodivmap/gbif_year_species\" + str(int(k_year))\n",
    "#                                + '/'+ k_species + \".geojson\", driver=\"GeoJSON\")\n",
    "#         print(str(int(k_year)) + \" done!\")\n",
    "    df.to_csv(\"Gbif Lesley.csv\", encoding='latin1')  \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = 'map_django/biodivmap/static/biodivmap/'\n",
    "taxonLevels = ['gbif','kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species']\n",
    "file_count = 0\n",
    "\n",
    "def pointCSVtoJSONPerYearPerTaxonLimited(filename, num_rows=0, testing_filter=False, lim=1000):\n",
    "    ##open dataframe from R script that removes duplicates based on lat, long, year & species\n",
    "    #and adds seasons\n",
    "    df = pd.read_csv(filename, low_memory=False)\n",
    "    #drop nan species\n",
    "    df = df.dropna(axis=0, subset=['species'])\n",
    "    #keep num_rows if num_rows > 0\n",
    "    if num_rows:\n",
    "        df = some(df, num_rows)\n",
    "    if testing_filter:\n",
    "        df = testingFilter(df)\n",
    "    #convert dataset keys to dataset names    \n",
    "    df = dataSetNamesFromKey(df)\n",
    "    print(\"Converted dataset keys to names!\")\n",
    "    #add redList designation\n",
    "    df['redList'] = df.apply(lambda x: redList(x), 1).values\n",
    "    print(\"Added redlist designation!\")\n",
    "    #add common names from ITS\n",
    "    df = addCommonNames(df)\n",
    "    print(\"Added common names!\")\n",
    "    df = df[['Winter', 'Spring', 'Summer', 'Fall', 'datasetName', 'common', \n",
    "                     'redList', 'decimalLatitude', 'decimalLongitude', 'year',\n",
    "            'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species']]\n",
    "    #add geometry from lat, long\n",
    "    geometry = [Point(xy) for xy in zip (df['decimalLongitude'], df['decimalLatitude'])]\n",
    "    #fix coordinate system\n",
    "    geo_df = gpd.GeoDataFrame(df, geometry = geometry, crs = {'init': 'epsg:4326'})\n",
    "    geo_df = geo_df.drop(['decimalLatitude', 'decimalLongitude'], 1)\n",
    "    #split into species per year as gejsons            \n",
    "    gb_year = geo_df.groupby(['year'])\n",
    "    os.mkdir(BASE_PATH + 'gbif/')\n",
    "    master_count = 0\n",
    "    dbCon = sqlite3.connect(\"map_django/db.sqlite3\")\n",
    "    cur   = dbCon.cursor()\n",
    "    for k_year, gp_year in gb_year:\n",
    "        gp_year = gp_year.drop(['year'], 1)\n",
    "        year_path = BASE_PATH + 'gbif/' + str(int(k_year)) + \"/\"\n",
    "        os.mkdir(year_path)\n",
    "        master_count += perTaxonWriter(0, gp_year, year_path, lim, cur, str(int(k_year)))\n",
    "        print(str(int(k_year)) + \" done!\")\n",
    "    dbCon.commit()\n",
    "    dbCon.close()\n",
    "    return geo_df, master_count\n",
    "\n",
    "def perTaxonWriter(taxonIndex, df, path, lim, cur, year):\n",
    "    if (taxonLevels[taxonIndex] == \"species\" or df.shape[0] <= lim):\n",
    "        for spec in list(df.species.unique()):\n",
    "            cur.execute(\"UPDATE species_year SET (%s) = ?  WHERE species = ?\" %(\"year_\"+ year),(path, spec,))\n",
    "        df.to_file(path + \"points\"+ \".geojson\", driver=\"GeoJSON\")\n",
    "        return 1\n",
    "    else:\n",
    "        gb_nextTaxonLevel = df.groupby([taxonLevels[taxonIndex+1]])\n",
    "        inner_count = 0\n",
    "        for k_nextTaxon, gp_nextTaxon in gb_nextTaxonLevel:\n",
    "            next_path = path + k_nextTaxon + \"/\"\n",
    "            os.mkdir(next_path)\n",
    "            inner_count+=perTaxonWriter(taxonIndex+1, gp_nextTaxon, next_path, lim, cur,year)\n",
    "        return inner_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted dataset keys to names!\n",
      "Added redlist designation!\n",
      "Added common names!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raghav/anaconda2/envs/jupy2/lib/python2.7/site-packages/geopandas/io/file.py:108: FionaDeprecationWarning: Use fiona.Env() instead.\n",
      "  with fiona.drivers():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1700 done!\n",
      "1800 done!\n",
      "1818 done!\n",
      "1856 done!\n",
      "1857 done!\n",
      "1859 done!\n",
      "1860 done!\n",
      "1872 done!\n",
      "1874 done!\n",
      "1875 done!\n",
      "1876 done!\n",
      "1877 done!\n",
      "1878 done!\n",
      "1879 done!\n",
      "1880 done!\n",
      "1881 done!\n",
      "1882 done!\n",
      "1883 done!\n",
      "1884 done!\n",
      "1885 done!\n",
      "1886 done!\n",
      "1887 done!\n",
      "1888 done!\n",
      "1889 done!\n",
      "1890 done!\n",
      "1891 done!\n",
      "1892 done!\n",
      "1893 done!\n",
      "1894 done!\n",
      "1895 done!\n",
      "1896 done!\n",
      "1897 done!\n",
      "1898 done!\n",
      "1899 done!\n",
      "1900 done!\n",
      "1901 done!\n",
      "1902 done!\n",
      "1903 done!\n",
      "1904 done!\n",
      "1905 done!\n",
      "1906 done!\n",
      "1907 done!\n",
      "1908 done!\n",
      "1909 done!\n",
      "1910 done!\n",
      "1911 done!\n",
      "1912 done!\n",
      "1913 done!\n",
      "1914 done!\n",
      "1915 done!\n",
      "1916 done!\n",
      "1917 done!\n",
      "1918 done!\n",
      "1919 done!\n",
      "1920 done!\n",
      "1921 done!\n",
      "1922 done!\n",
      "1923 done!\n",
      "1924 done!\n",
      "1925 done!\n",
      "1926 done!\n",
      "1927 done!\n",
      "1928 done!\n",
      "1929 done!\n",
      "1930 done!\n",
      "1931 done!\n",
      "1932 done!\n",
      "1933 done!\n",
      "1934 done!\n",
      "1935 done!\n",
      "1936 done!\n",
      "1937 done!\n",
      "1938 done!\n",
      "1939 done!\n",
      "1940 done!\n",
      "1941 done!\n",
      "1942 done!\n",
      "1943 done!\n",
      "1944 done!\n",
      "1945 done!\n",
      "1946 done!\n",
      "1947 done!\n",
      "1948 done!\n",
      "1949 done!\n",
      "1950 done!\n",
      "1951 done!\n",
      "1952 done!\n",
      "1953 done!\n",
      "1954 done!\n",
      "1955 done!\n",
      "1956 done!\n",
      "1957 done!\n",
      "1958 done!\n",
      "1959 done!\n",
      "1960 done!\n",
      "1961 done!\n",
      "1962 done!\n",
      "1963 done!\n",
      "1964 done!\n",
      "1965 done!\n",
      "1966 done!\n",
      "1967 done!\n",
      "1968 done!\n",
      "1969 done!\n",
      "1970 done!\n",
      "1971 done!\n",
      "1972 done!\n",
      "1973 done!\n",
      "1974 done!\n",
      "1975 done!\n",
      "1976 done!\n",
      "1977 done!\n",
      "1978 done!\n",
      "1979 done!\n",
      "1980 done!\n",
      "1981 done!\n",
      "1982 done!\n",
      "1983 done!\n",
      "1984 done!\n",
      "1985 done!\n",
      "1986 done!\n",
      "1987 done!\n",
      "1988 done!\n",
      "1989 done!\n",
      "1990 done!\n",
      "1991 done!\n",
      "1992 done!\n",
      "1993 done!\n",
      "1994 done!\n",
      "1995 done!\n",
      "1996 done!\n",
      "1997 done!\n",
      "1998 done!\n",
      "1999 done!\n",
      "2000 done!\n",
      "2001 done!\n",
      "2002 done!\n",
      "2003 done!\n",
      "2004 done!\n",
      "2005 done!\n",
      "2006 done!\n",
      "2007 done!\n",
      "2008 done!\n",
      "2009 done!\n",
      "2010 done!\n",
      "2011 done!\n",
      "2012 done!\n",
      "2013 done!\n",
      "2014 done!\n",
      "2015 done!\n",
      "2016 done!\n",
      "2017 done!\n",
      "2018 done!\n",
      "2019 done!\n"
     ]
    }
   ],
   "source": [
    "geo_df, count = pointCSVtoJSONPerYearPerTaxonLimited(\"GBif_R.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3406"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum = 0\n",
    "for key,gp in geo_df.groupby([\"year\"]):\n",
    "    sum+=gp.species.unique().shape[0]\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointCSVtoJSONPerYearPerSpecies(filename, num_rows=0, testing_filter=False):\n",
    "    ##open dataframe from R script that removes duplicates based on lat, long, year & species\n",
    "    #and adds seasons\n",
    "    df = pd.read_csv(filename, low_memory=False)\n",
    "    #drop nan species\n",
    "    df = df.dropna(axis=0, subset=['species'])\n",
    "    #keep num_rows if num_rows > 0\n",
    "    if num_rows:\n",
    "        df = some(df, num_rows)\n",
    "    if testing_filter:\n",
    "        df = testingFilter(df)\n",
    "    #convert dataset keys to dataset names    \n",
    "    df = dataSetNamesFromKey(df)\n",
    "    print(\"Converted dataset keys to names!\")\n",
    "    #add redList designation\n",
    "    df['redList'] = df.apply(lambda x: redList(x), 1).values\n",
    "    print(\"Added redlist designation!\")\n",
    "    #add common names from ITIS\n",
    "    df = addCommonNames(df)\n",
    "    print(\"Added common names!\")\n",
    "    df = df[['species', 'Winter', 'Spring', 'Summer', 'Fall', 'datasetName', 'common', \n",
    "                     'redList', 'decimalLatitude', 'decimalLongitude', 'year']]\n",
    "    #add geometry from lat, long\n",
    "    geometry = [Point(xy) for xy in zip (df['decimalLongitude'], df['decimalLatitude'])]\n",
    "    #fix coordinate system\n",
    "    geo_df = gpd.GeoDataFrame(df, geometry = geometry, crs = {'init': 'epsg:4326'})\n",
    "    geo_df = geo_df.drop(['decimalLatitude', 'decimalLongitude'], 1)\n",
    "    #split into species per year as gejsons            \n",
    "    gb_year = geo_df.groupby(['year'])\n",
    "    for k_year, gp_year in gb_year:\n",
    "        gp_year = gp_year.drop(['year'], 1)\n",
    "        gp_species = gp_year.groupby(['species'])\n",
    "        os.mkdir('leaflet/gbif_year_species/' + str(int(k_year)))\n",
    "        for k_species, gp_species in gp_species:\n",
    "            gp_species.to_file(\"map_django/biodivmap/static/biodivmap/gbif_year_species\" + str(int(k_year))\n",
    "                               + '/'+ k_species + \".geojson\", driver=\"GeoJSON\")\n",
    "        print(str(int(k_year)) + \" done!\")\n",
    "        \n",
    "    return geo_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df = pointCSVtoJSONPerYearPerSpecies(\"Gbif_R.csv\", testing_filter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Gbif_R.csv\", low_memory=False)\n",
    "df = df.dropna(subset = ['year'])\n",
    "df.year = df['year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_series = df.groupby(['year']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_counts = list(zip(gp_series.index, gp_series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(gp_series.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbCon = sqlite3.connect(\"map_django/db.sqlite3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur   = dbCon.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INSERT SPECIES\n",
    "for spec in list(geo_df.species.unique()):\n",
    "    cur.execute(\"INSERT INTO species_year (species) VALUES (?)\",(spec,))\n",
    "dbCon.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbCon.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"UPDATE species_year SET (%s) = ?  WHERE species = ?\" %(\"year_1700\"),(\"path/\", \"Abagrotis baueri\",))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_query = \"\"\"CREATE TABLE species_year (\n",
    "    species TEXT NOT NULL PRIMARY KEY,\"\"\"\n",
    "\n",
    "for year in years:\n",
    "    if (year == 2019):\n",
    "        col_query+=\"\"\"year_\"\"\" + str(year) + \"\"\" INTEGER DEFAULT 0\"\"\"\n",
    "    else:\n",
    "        col_query+=\"\"\"year_\"\"\" + str(year) + \"\"\" INTEGER DEFAULT 0,\"\"\"\n",
    "    \n",
    "col_query += \"\"\");\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(col_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "dbCon.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('map_django/years.pkl', 'wb') as f:\n",
    "    pickle.dump(years, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pointCSVtoJSON(filename, num_rows=0):\n",
    "    df = pd.read_csv(filename, delimiter=\"\\t\", low_memory=False)\n",
    "    if (num_rows):\n",
    "        df = some(df, num_rows)\n",
    "    geometry = [Point(xy) for xy in zip (df['decimalLongitude'], df['decimalLatitude'])]\n",
    "    geo_df = gpd.GeoDataFrame(df, geometry = geometry, crs = {'init': 'epsg:4326'})\n",
    "    \n",
    "    # gbif_geo_df = gbif_geo_df[gbif_geo_df['species'].isin(red_sciNames)]\n",
    "#     geo_df['redList'] = geo_df.apply(lambda x: int(x['species']in red_sciNames.values), 1).values\n",
    "    geo_df['redList'] = geo_df.apply(lambda x: redList(x, red_sciNames), 1).values\n",
    "    \n",
    "    \n",
    "    # Common Names ITIS\n",
    "    # # Get names of all tables\n",
    "    # c = conn.cursor()\n",
    "    # c.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    # print(c.fetchall())\n",
    "    \n",
    "    geo_df = addCommonNames(geo_df)\n",
    "    geo_df.to_file(\"leaflet/\" + filename.split('.')[0] + '.geojson', driver=\"GeoJSON\")\n",
    "    \n",
    "    return geo_df\n",
    "\n",
    "def pointCSVtoJSONPerYear(filename, num_rows=0):\n",
    "    df = pd.read_csv(filename, low_memory=False)\n",
    "    if (num_rows):\n",
    "        df = some(df, num_rows)\n",
    "    geometry = [Point(xy) for xy in zip (df['decimalLongitude'], df['decimalLatitude'])]\n",
    "    geo_df = gpd.GeoDataFrame(df, geometry = geometry, crs = {'init': 'epsg:4326'})\n",
    "    \n",
    "    # gbif_geo_df = gbif_geo_df[gbif_geo_df['species'].isin(red_sciNames)]\n",
    "#     geo_df['redList'] = geo_df.apply(lambda x: int(x['species']in red_sciNames.values), 1).values\n",
    "    geo_df['redList'] = geo_df.apply(lambda x: redList(x, red_sciNames), 1).values\n",
    "    \n",
    "    \n",
    "    # Common Names ITIS\n",
    "    # # Get names of all tables\n",
    "    # c = conn.cursor()\n",
    "    # c.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "    # print(c.fetchall())\n",
    "    \n",
    "    geo_df = addCommonNames(geo_df)\n",
    "    \n",
    "    geo_df = geo_df.drop(['decimalLatitude', 'decimalLongitude'], 1)\n",
    "    \n",
    "    gb = geo_df.groupby(['year'])\n",
    "\n",
    "    for k, gp in gb:\n",
    "#        gp.drop(['year'], 1)\n",
    "       gp.to_file(\"leaflet/gbif/\" + str(int(k)) + \".geojson\", driver=\"GeoJSON\")\n",
    "    return geo_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_df = pointCSVtoJSONPerYear('GBif Trim June19.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"GBif Trim June19.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHP data to geoJSON\n",
    "def fix_crs(map_ob):\n",
    "    return map_ob.to_crs({'init': 'epsg:4326'})\n",
    "# str_map = gpd.read_file(\"ecological_reserves/BC_Eco_Reserves.shp\")\n",
    "str_map = gpd.read_file(\"MVSEI2014/MVSEI2014.shp\")\n",
    "\n",
    "str_map = fix_crs(str_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GeoJSON does not support multipolygon. Doesn't work\n",
    "# str_map.to_file(\"leaflet/UBC_poly.geojson\", driver=\"GeoJSON\")\n",
    "\n",
    "# # fiona doesn't work\n",
    "# import fiona\n",
    "# import json\n",
    "\n",
    "# with fiona.open('ecological_reserves/BC_Eco_Reserves.shp') as source:\n",
    "#     records = list(source)\n",
    "# geo_json = {\"type\": \"FeatureCollection\",\"features\": records}\n",
    "# with open('leaflet/UBC_poly.geojson', 'w') as fp:\n",
    "#     json.dump(geo_json, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert multipolygon to single polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom_series = str_map.geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def geom_apply(x):\n",
    "    try:\n",
    "        return list(x)\n",
    "    except TypeError:\n",
    "        return [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom_series = geom_series.apply(geom_apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom_series = geom_series.apply(pd.Series).stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Quick but loses properties\n",
    "# list_poly = list(geom_series)\n",
    "# geoms = GeometryCollection(list_poly)\n",
    "# geo_file = geojson.dumps(geoms)\n",
    "# with open(\"leaflet/UBC_poly.geojson\", \"w\") as text_file:\n",
    "#     text_file.write(geo_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Careful! Deep copy required here to avoid chaining\n",
    "\n",
    "df = pd.DataFrame(columns=str_map.columns)\n",
    "for ind, poly in geom_series.iteritems():\n",
    "    curr_row = str_map.loc[ind[0]].copy(deep=True)\n",
    "    curr_row['geometry'] = poly\n",
    "    df = df.append(curr_row)\n",
    "\n",
    "df_gpd = gpd.GeoDataFrame(df,geometry = df.geometry, crs = {'init': 'epsg:4326'})\n",
    "df_gpd.to_file(\"leaflet/SEI.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHP data to geoJSON\n",
    "def fix_crs(map_ob):\n",
    "    return map_ob.to_crs({'init': 'epsg:4326'})\n",
    "# str_map = gpd.read_file(\"ecological_reserves/BC_Eco_Reserves.shp\")\n",
    "str_map = gpd.read_file(\"MVSEI2014/MVSEI2014.shp\")\n",
    "\n",
    "str_map = fix_crs(str_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_map.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_map.Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_map = str_map[['SEI_PolyNb', 'Comp1Lgnd', 'geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_map.to_file(\"leaflet/SEI.geojson\", driver=\"GeoJSON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_gdf_geometry(gdf, geom_type):\n",
    "    geometry = gdf.geometry\n",
    "    flattened_geometry = []\n",
    "\n",
    "    flattened_gdf = gpd.GeoDataFrame()\n",
    "\n",
    "    for geom in geometry:\n",
    "        if geom.type in ['GeometryCollection', 'MultiPoint', 'MultiLineString', 'MultiPolygon']:\n",
    "            for subgeom in geom:\n",
    "                if subgeom.type==geom_type:\n",
    "                    flattened_geometry.append(subgeom)\n",
    "        else:\n",
    "            if geom.type==geom_type:\n",
    "                flattened_geometry.append(geom)\n",
    "\n",
    "    flattened_gdf.geometry=flattened_geometry\n",
    "\n",
    "    return flattened_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = flatten_gdf_geometry(str_map, 'Polygon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jupy2)",
   "language": "python",
   "name": "jupy2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
