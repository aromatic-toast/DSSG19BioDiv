<<<<<<< HEAD
sara_designations <- c(sara_designations, "Data Deficient")
}else
sara_designations <- c(sara_designations, "Not Listed")
}
# add the sara designations vector to the species_info dataframe
species_info$sara_designations <- sara_designations
View(head(species_info))
write.csv(species_info, file = "Taxonomy_Freq.csv", row.names = FALSE)
getwd()
# read in the specific species info data frame
species_info <- read.csv("Taxonomy_Freq.csv", stringsAsFactors = F)
View(head(species_info))
IUCN_binary <- c()
for (i in species_info$IUCN_binary){
if(is.na(i)){
IUCN_redList_binary <- c(IUCN_redList_binary, 0)
}else {
IUCN_redList_binary <- c(IUCN_redList_binary, 1)
}
}
# add the binary vector to the species_info dataframe
species_info$IUCN_binary <- IUCN_redList_binary
# read in IUCN red list
IUCN <- read.csv("~/Desktop/datasets/IUCN_redlist.csv",
stringsAsFactors = F,
na.strings = c("", " "))
# read in the full BC rainbow list dataframe
rainbow_list <- read.csv("~/Desktop/datasets/bc_rainbow_list.tsv",
sep = "\t",
stringsAsFactors = F,
na.strings = c("", " "))
for (i in species_info$IUCN_binary){
if(is.na(i)){
IUCN_redList_binary <- c(IUCN_redList_binary, 0)
}else {
IUCN_redList_binary <- c(IUCN_redList_binary, 1)
}
}
# add the binary vector to the species_info dataframe
species_info$IUCN_binary <- IUCN_redList_binary
for (i in species_info$BC.List){
if(i == "Red"){
bc_red_binary <- c(bc_red_binary, 1)
}else {
bc_red_binary <- c(bc_red_binary, 0)
}
}
# add the bc_red list binary vector to the species_info dataframe
species_info$bc_red_binary <- bc_red_binary
for (i in species_info$BC.List){
if(i == "Blue"){
bc_blue_binary <- c(bc_blue_binary, 1)
}else {
bc_blue_binary <- c(bc_blue_binary, 0)
}
}
# add the bc_blue list binary vector to the species_info dataframe
species_info$bc_blue_binary <- bc_blue_binary
IUCN_binary <- c()
for (i in species_info$redList){
if(is.na(i)){
IUCN_redList_binary <- c(IUCN_redList_binary, 0)
}else {
IUCN_redList_binary <- c(IUCN_redList_binary, 1)
}
}
# add the binary vector to the species_info dataframe
species_info$IUCN_binary <- IUCN_redList_binary
# create a binary vector for BC red listed species
# 1 = entry is a string "red" , 0 = entry is anything that is not string "red"
bc_red_binary <- c()
for (i in species_info$bc_list){
if(i == "Red"){
bc_red_binary <- c(bc_red_binary, 1)
}else {
bc_red_binary <- c(bc_red_binary, 0)
}
}
# add the bc_red list binary vector to the species_info dataframe
species_info$bc_red_binary <- bc_red_binary
# create a binary vector for BC blue listed species
# 1 = entry is a string "Blue" , 0 = entry is anything that is not string "Blue"
bc_blue_binary <- c()
for (i in species_info$bc_list){
if(i == "Blue"){
bc_blue_binary <- c(bc_blue_binary, 1)
}else {
bc_blue_binary <- c(bc_blue_binary, 0)
}
}
# add the bc_blue list binary vector to the species_info dataframe
species_info$bc_blue_binary <- bc_blue_binary
# create a binary vector for BC endemic listed species
# these species are being tagged as only living in BC
# 1 = entry is a string "Y" , 0 = entry is anything that is not string "Y"
bc_endemic_binary <- c()
for (i in species_info$Endemic){
if(i == "Y"){
bc_endemic_binary <- c(bc_endemic_binary, 1)
}else {
bc_endemic_binary <- c(bc_endemic_binary, 0)
}
}
# add the bc_blue list binary vector to the species_info dataframe
species_info$bc_endemic_binary <- bc_endemic_binary
######  create a binary vector for pollinators ########
# pollinator info obtained from wikipedia
# Note: the following species will be tagged as pollinators
# the following are all hummingbirds
# - Archilochus alexandri
# - Calypte anna
# - Calypte costae
# - Selasphorus rufus
# - Selasphorus sasin
# - Stellula calliope
# the following insect genera will be tagged as pollinators
# - Apis (honeybees)
# - Bombus (bumblebees)
# the following insect families will be tagged as pollinators
# family=="Masarinae" = pollen wasps
#   family=="Formicidae"  = ants
#   family=="Bombyliidae" = bee flies
#   family=="Syrphidae" = hover flies
#   family=="Buprestidae" = beetle
#   family=="Cantharidae" = beetle
#   family=="Carambycidae" = beetle
#   family=="Cleridae" = beetle
#   family=="Dermestidae" = beetle
#   family=="Lycidae" = beetle
#   family=="Melyridae" = beetle
#   family=="Mordellidae"  = beetle
#   family=="Nitidulidae"  = beetle
#   family=="Scarabeidae" = beetle
# the following insect Order will be tagged as pollinators
# they are moths and butterflies
# Lepidoptera
# create a pollinators dataframe
pollinators <- species_info %>%
filter(genus=="Apis" |
genus=="Bombus" |
order=="Lepidoptera" |
family=="Masarinae" |
family=="Formicidae" |
family=="Bombyliidae" |
family=="Syrphidae" |
family=="Buprestidae" |
family=="Cantharidae" |
family=="Carambycidae" |
family=="Cleridae" |
family=="Dermestidae" |
family=="Lycidae" |
family=="Melyridae" |
family=="Mordellidae" |
family=="Nitidulidae" |
family=="Scarabeidae" |
simplified_names == "Archilochus alexandri" |
simplified_names == "Calypte anna" |
simplified_names == "Calypte costae" |
simplified_names == "Selasphorus rufus" |
simplified_names == "Selasphorus sasin" |
simplified_names == "Stellula calliope")
# create a binary vector for pollinator species
# 1 = species is in the pollinator list above , 0 = species is not found in the pollinator list
pollinator_binary <- c()
for (i in species_info$simplified_names){
if(i %in% pollinators$simplified_names){
pollinator_binary <- c(pollinator_binary, 1)
}else {
pollinator_binary <- c(pollinator_binary, 0)
}
}
# add the pollinator binary vector to the species_info dataframe
species_info$pollinator_binary <- pollinator_binary
# rename the pollinator column to pollinator_binary
#species_info <- species_info %>% rename(pollinator_binary = pollinator)
# create a binary vector for SARA listed species
# 1 = species is SARA listed, 0 = species is not on the SARA list
sara_binary <- c()
for (i in species_info$SARA){
if(is.na(i)){
sara_binary <- c(sara_binary, 0)
}else {
sara_binary <- c(sara_binary, 1)
}
}
# add the SARA binary vector to the species_info dataframe
species_info$sara_binary <- sara_binary
# Turn the SARA designations into a human readable vector
sara_designations <- c()
for(i in species_info$SARA){
if(is.na(i)){
sara_designations <- c(sara_designations, "Not Listed")
} else if(length(grep("XX", i))> 0){
sara_designations <- c(sara_designations, "Extinct")
}else if(length(grep("XT", i))> 0){
sara_designations <- c(sara_designations, "Extirpated")
}else if(length(grep("E", i))> 0){
sara_designations <- c(sara_designations, "Endangered")
}else if(length(grep("T", i))> 0){
sara_designations <- c(sara_designations, "Threatened")
}else if(length(grep("SC", i))> 0){
sara_designations <- c(sara_designations, "Special Concern")
}else if(length(grep("NAR", i))> 0){
sara_designations <- c(sara_designations, "Not At Risk")
}else if(length(grep("DD", i))> 0){
sara_designations <- c(sara_designations, "Data Deficient")
}else
sara_designations <- c(sara_designations, "Not Listed")
}
# add the sara designations vector to the species_info dataframe
species_info$sara_designations <- sara_designations
bc_red_binary <- c()
for (i in species_info$bc_list){
if(i == "Red"){
print(i)
bc_red_binary <- c(bc_red_binary, 1)
}else {
bc_red_binary <- c(bc_red_binary, 0)
}
}
bc_red_binary <- c()
for (i in species_info$bc_list){
print(i)
if(i == "Red"){
bc_red_binary <- c(bc_red_binary, 1)
}else {
bc_red_binary <- c(bc_red_binary, 0)
}
}
NA=="red"
bc_red_binary <- c()
for (i in species_info$bc_list){
if(is.na(i)){
bc_red_binary <- c(bc_red_binary, 0)
}else if(i == "Red"){
bc_red_binary <- c(bc_red_binary, 1)
}else {
bc_red_binary <- c(bc_red_binary, 0)
}
}
# add the bc_red list binary vector to the species_info dataframe
species_info$bc_red_binary <- bc_red_binary
bc_blue_binary <- c()
for (i in species_info$bc_list){
if(is.na(i)){
bc_blue_binary <- c(bc_blue_binary, 0)
}else if(i == "Blue"){
bc_blue_binary <- c(bc_blue_binary, 1)
}else {
bc_blue_binary <- c(bc_blue_binary, 0)
}
}
# add the bc_blue list binary vector to the species_info dataframe
species_info$bc_blue_binary <- bc_blue_binary
bc_endemic_binary <- c()
for (i in species_info$Endemic){
if(is.na(i)){
bc_endemic_binary <- c(bc_endemic_binary, 0)
}
if(i == "Y"){
bc_endemic_binary <- c(bc_endemic_binary, 1)
}else {
bc_endemic_binary <- c(bc_endemic_binary, 0)
}
}
for (i in species_info$Endemic){
if(is.na(i)){
bc_endemic_binary <- c(bc_endemic_binary, 0)
}else if(i == "Y"){
bc_endemic_binary <- c(bc_endemic_binary, 1)
}else {
bc_endemic_binary <- c(bc_endemic_binary, 0)
}
}
# add the bc_blue list binary vector to the species_info dataframe
species_info$bc_endemic_binary <- bc_endemic_binary
length(bc_endemic_binary)
nrow(species_info)
bc_endemic_binary <- c()
for (i in species_info$Endemic){
if(is.na(i)){
bc_endemic_binary <- c(bc_endemic_binary, 0)
}else if(i == "Y"){
bc_endemic_binary <- c(bc_endemic_binary, 1)
}else {
bc_endemic_binary <- c(bc_endemic_binary, 0)
}
}
length(bc_endemic_binary)
# add the bc_blue list binary vector to the species_info dataframe
species_info$bc_endemic_binary <- bc_endemic_binary
# create a pollinators dataframe
pollinators <- species_info %>%
filter(genus=="Apis" |
genus=="Bombus" |
order=="Lepidoptera" |
family=="Masarinae" |
family=="Formicidae" |
family=="Bombyliidae" |
family=="Syrphidae" |
family=="Buprestidae" |
family=="Cantharidae" |
family=="Carambycidae" |
family=="Cleridae" |
family=="Dermestidae" |
family=="Lycidae" |
family=="Melyridae" |
family=="Mordellidae" |
family=="Nitidulidae" |
family=="Scarabeidae" |
simplified_names == "Archilochus alexandri" |
simplified_names == "Calypte anna" |
simplified_names == "Calypte costae" |
simplified_names == "Selasphorus rufus" |
simplified_names == "Selasphorus sasin" |
simplified_names == "Stellula calliope")
if(i %in% pollinators$simplified_names){
pollinator_binary <- c(pollinator_binary, 1)
}else {
pollinator_binary <- c(pollinator_binary, 0)
}
# add the pollinator binary vector to the species_info dataframe
species_info$pollinator_binary <- pollinator_binary
sara_binary <- c(sara_binary, 1)
# add the SARA binary vector to the species_info dataframe
species_info$sara_binary <- sara_binary
sara_designations <- c()
for(i in species_info$SARA){
if(is.na(i)){
sara_designations <- c(sara_designations, "Not Listed")
} else if(length(grep("XX", i))> 0){
sara_designations <- c(sara_designations, "Extinct")
}else if(length(grep("XT", i))> 0){
sara_designations <- c(sara_designations, "Extirpated")
}else if(length(grep("E", i))> 0){
sara_designations <- c(sara_designations, "Endangered")
}else if(length(grep("T", i))> 0){
sara_designations <- c(sara_designations, "Threatened")
}else if(length(grep("SC", i))> 0){
sara_designations <- c(sara_designations, "Special Concern")
}else if(length(grep("NAR", i))> 0){
sara_designations <- c(sara_designations, "Not At Risk")
}else if(length(grep("DD", i))> 0){
sara_designations <- c(sara_designations, "Data Deficient")
}else
sara_designations <- c(sara_designations, "Not Listed")
}
# add the sara designations vector to the species_info dataframe
species_info$sara_designations <- sara_designations
View(head(species_info))
write.csv(species_info, file = "Taxonomy_Freq.csv", row.names = FALSE)
# read in the specific species info data frame
species_info <- read.csv("Taxonomy_Freq.csv", stringsAsFactors = F)
View(head(species_info))
# read in the specific species info data frame
species_info <- read.csv("Taxonomy_Freq.csv", stringsAsFactors = F)
View(head(species_info))
# read in IUCN red list
IUCN <- read.csv("~/Desktop/datasets/IUCN_redlist.csv",
stringsAsFactors = F,
na.strings = c("", " "))
View(head(IUCN))
(tidyverse)
getwd()
install.packages()
install.packages("tidyverse")
# prepare the species info dataframe to contain custom tags for shiny app plotting
library(tidyverse)
for (i in species_info$redList){
if(is.na(i)){
IUCN_binary <- c(IUCN_binary, 0)
}else {
IUCN_binary <- c(IUCN_binary, 1)
}
}
IUCN_binary <- c()
for (i in species_info$redList){
if(is.na(i)){
IUCN_binary <- c(IUCN_binary, 0)
}else {
IUCN_binary <- c(IUCN_binary, 1)
}
}
# add the binary vector to the species_info dataframe
species_info$IUCN_binary <- IUCN_binary
View(head(species_info))
write.csv(species_info, file = "Taxonomy_Freq.csv", row.names = FALSE)
# read in the specific species info data frame
species_info <- read.csv("Taxonomy_Freq.csv", stringsAsFactors = F)
View(head(species_info))
poly_index <- c(1:24)
poly_type1 <- rep(c("OF", "MF", "WD", "RI", "WN", "HB", "SV", "ES", "IT", "FW", "AP"), 2)
poly_comp1 <- c(rep(1, 12), 0.8, rep(1, 11))
poly_type2 <- c("WD", rep(NA, 11))
poly_comp2 <- c(0.2, rep(NA, 11))
df_polygons <- data.frame(c(poly_index, poly_type1, poly_comp1, poly_type2, poly_comp2))
View(df_polygons)
df_polygons <- data.frame(poly_index, poly_type1, poly_comp1, poly_type2, poly_comp2)
poly_index <- c(1:24)
poly_type1 <- rep(c("OF", "MF", "WD", "RI", "WN", "HB", "SV", "ES", "IT", "FW", "AP"), 2)
poly_comp1 <- c(rep(1, 12), 0.8, rep(1, 11))
poly_type2 <- c(rep(NA, 12),"WD", rep(NA, 11))
poly_comp2 <- c(rep(NA, 12),0.2, rep(NA, 11))
df_polygons <- data.frame(poly_index, poly_type1, poly_comp1, poly_type2, poly_comp2)
length(poly_comp1)
length(poly_comp2)
poly_type1
length(poly_type1)
poly_type2
lenght(poly_type2)
length(poly_type2)
poly_index <- c(1:24)
poly_type1 <- rep(c("OF", "MF", "WD", "RI", "WN", "HB", "SV", "ES", "IT", "FW", "AP", "ME"), 2)
poly_comp1 <- c(rep(1, 12), 0.8, rep(1, 11))
poly_type2 <- c(rep(NA, 12),"WD", rep(NA, 11))
poly_comp2 <- c(rep(NA, 12),0.2, rep(NA, 11))
df_polygons <- data.frame(poly_index, poly_type1, poly_comp1, poly_type2, poly_comp2)
View(df_polygons)
poly_index <- c(1:12)
poly_type1 <- rep(c("OF", "MF", "WD", "RI", "WN", "HB", "SV", "ES", "IT", "FW", "AP", "ME"), 2)
poly_comp1 <- c(rep(1, 12), 0.8, rep(1, 11))
poly_type2 <- c(rep(NA, 12),"WD", rep(NA, 11))
poly_comp2 <- c(rep(NA, 12),0.2, rep(NA, 11))
df_polygons <- data.frame(poly_index, poly_type1, poly_comp1, poly_type2, poly_comp2)
View(df_polygons)
poly_index <- c(1:24)
poly_type1 <- rep(c("OF", "MF", "WD", "RI", "WN", "HB", "SV", "ES", "IT", "FW", "AP", "ME"), 2)
poly_comp1 <- c(rep(1, 12), 0.8, rep(1, 11))
poly_type2 <- c(rep(NA, 12),"WD", rep(NA, 11))
poly_comp2 <- c(rep(NA, 12),0.2, rep(NA, 11))
df_polygons <- data.frame(poly_index, poly_type1, poly_comp1, poly_type2, poly_comp2)
View(df_polygons)
# read in the specific species info data frame
species_info <- read.csv("Taxonomy_Freq.csv", stringsAsFactors = F)
View(head(species_info))
View(head(species_info), 50)
View(head(species_info, 50))
species <- c(rep("Abagrotis baueri", 3), rep("Abies balsamea", 4), "Abies bracteata", "Acanthocalyx alba", rep("Acacia pravissima", 3),
rep("Abagrotis baueri", 3), "Acaena splendens", "Abrostola urentis", rep("Abies balsamea", 2), "Abies bracteata", "Acanthocalyx alba", rep("Acacia pravissima", 3))
df_polygons <- data.frame(poly_index, poly_type1, poly_comp1, poly_type2, poly_comp2, species)
View(df_polygons)
library(tidyverse)
# group by polygon type
group_by(df_polygons, poly_type1)
# group by polygon type
View(group_by(df_polygons, poly_type1))
class(group_by(df_polygons, poly_type1))
# group by polygon type
View(group_by(df_polygons, poly_type1))
# group by polygon type
by_poly_type1 <- group_by(df_polygons, poly_type1)
by_poly_type1 %>% summarise(n())
?summarise
poly_index <- c(1:24, 1)
poly_type1 <- c(rep(c("OF", "MF", "WD", "RI", "WN", "HB", "SV", "ES", "IT", "FW", "AP", "ME"), 2), "OF")
poly_comp1 <- c(rep(1, 12), 0.8, rep(1, 11),1)
poly_type2 <- c(rep(NA, 12),"WD", rep(NA, 11), NA)
poly_comp2 <- c(rep(NA, 12),0.2, rep(NA, 11), NA)
species <- c(rep("Abagrotis baueri", 3), rep("Abies balsamea", 4), "Abies bracteata", "Acanthocalyx alba", rep("Acacia pravissima", 3),
rep("Abagrotis baueri", 3), "Acaena splendens", "Abrostola urentis", rep("Abies balsamea", 2), "Abies bracteata", "Acanthocalyx alba", rep("Acacia pravissima", 3),
"Acanthocalyx alba")
# create the whole species_SEI dataframe
df_polygons <- data.frame(poly_index, poly_type1, poly_comp1, poly_type2, poly_comp2, species)
# group by polygon type
by_poly_type1 <- group_by(df_polygons, poly_type1)
by_poly_type1 %>% summarise(num_polys = unique(poly_index)
by_poly_type1 %>% summarise(num_polys = unique(poly_index))
by_poly_type1 %>% num_polys = unique(poly_index)
by_poly_type1 %>% unique(poly_index)
by_poly_type1 %>% summarise(num_polys = unique(poly_index))
filter(df_polygons, poly_type1=="OF") %>% select(poly_index) %>% unique()
class(filter(df_polygons, poly_type1=="OF") %>% select(poly_index) %>% unique())
filter(df_polygons, species == "Abagrotis baueri")
# filter by species
filter(df_polygons, species == "Abagrotis baueri") %>%
filter(poly_type1 == "OF")
# filter by species
filter(df_polygons, species == "Abagrotis baueri") %>%
filter(poly_type1 == "OF") %>% length(poly_index)
# filter by species
filter(df_polygons, species == "Abagrotis baueri") %>%
filter(poly_type1 == "OF") %>%
select(poly_index) %>%
length()
length(df_polygons)
ncol(df_polygons)
# filter by species
filter(df_polygons, species == "Abagrotis baueri") %>%
filter(poly_type1 == "OF") %>%
select(poly_index) %>%
unique(.)
# filter by species
filter(df_polygons, species == "Abagrotis baueri") %>%
filter(poly_type1 == "OF") %>%
select(poly_index) %>%
length(unique(.))
?select
# filter by species
d <- filter(df_polygons, species == "Abagrotis baueri") %>%
filter(poly_type1 == "OF")
d["poly_index"]
d[["poly_index"]]
unique(d[["poly_index"]])
length(unique(d[["poly_index"]]))
# filter by species
filter(df_polygons, species == "Abagrotis baueri") %>%
filter(poly_type1 == "OF") %>%
select(poly_index) %>%
nrow()
########## SEI and species prediction #########
df_sei_polygons <- read.csv("sei_data.csv", stringsAsFactors = F)
View(head(df_sei_polygons))
=======
# Note: "data" must have exactly columns "member", "year", and "n"
add_zeros1 = function(data) {
if (class(data)[1] != "data.frame") {data = as.data.frame(data)}
for (mem in unique(data$member)) {
if(nrow(data[which(data$member==mem),]) > 1) {
min = min(data$year[which(data$member == mem)], na.rm = T)
max = max(data$year[which(data$member == mem)], na.rm = T)
for (yea in (min+1):(max-1)) {
if (!(yea %in% data$year[which(data$member == mem)])) {
data = rbind(data, data.frame(member = mem, year = as.integer(yea), n = as.integer(0), stringsAsFactors = F))
}
}
}
}
return(data)
}
# For custom tag data, aggregation is done on a column-by-column (tag-by-tag) basis, necessitating a diferent function that accepts the maximum year range of the original data so that it knows to add zeros beyond the range of an individual tag (on the plus side, we don't need to worry about single-data-point cases)
add_zeros2 = function(data, min, max) {
missing_years = setdiff(min:max, data$year)
if (length(missing_years) != 0) {
x = cbind(data.frame(setdiff(min:max, data$year), 0))
colnames(x) = c("year", "n")
return(rbind(data, x))
}
else {return(data)}
}
# Function for removing decimal places for the sake of labels
no_dec = function(x) {sprintf("%.0f", x)}
runApp()
runApp()
runApp()
runApp()
runApp()
library(shiny)
library(tidyverse)
library(stringr)
# Read in the data
dfsp <- read.csv("Taxonomy_Freq.csv", stringsAsFactors = F)
df_orig <- readRDS("gbif_summary.rds")
# Record which columns in dfsp should be treated as taxonomies and which should be treated as custom tags
tax_columns = which(colnames(dfsp) %in% c("kingdom","phylum","order","class","family","genus","species"))
tax_list = colnames(dfsp)[tax_columns]
names(tax_list) = str_to_title(tax_list)
tag_columns = grep("*_binary", colnames(dfsp))
tag_list = colnames(dfsp)[tag_columns]
names(tag_list) = sub("*_binary", "", colnames(dfsp)[tag_columns])
names(tag_list) = sub("_", " ", names(tag_list))
# create a dataframe containing total num of observations for each year
yearly_obs <- group_by(df_orig, year) %>% tally() %>% drop_na()
##~~ FUNCTIONS ~~##
# Function for adding 0-value rows to aggregate tally dataframes to fill out the years between the first and last years
# Note: "data" must have exactly columns "member", "year", and "n"
add_zeros1 = function(data) {
if (class(data)[1] != "data.frame") {data = as.data.frame(data)}
for (mem in unique(data$member)) {
if(nrow(data[which(data$member==mem),]) > 1) {
min = min(data$year[which(data$member == mem)], na.rm = T)
max = max(data$year[which(data$member == mem)], na.rm = T)
for (yea in (min+1):(max-1)) {
if (!(yea %in% data$year[which(data$member == mem)])) {
data = rbind(data, data.frame(member = mem, year = as.integer(yea), n = as.integer(0), stringsAsFactors = F))
}
}
}
}
return(data)
}
# For custom tag data, aggregation is done on a column-by-column (tag-by-tag) basis, necessitating a diferent function that accepts the maximum year range of the original data so that it knows to add zeros beyond the range of an individual tag (on the plus side, we don't need to worry about single-data-point cases)
add_zeros2 = function(data, min, max) {
missing_years = setdiff(min:max, data$year)
if (length(missing_years) != 0) {
x = cbind(data.frame(setdiff(min:max, data$year), 0))
colnames(x) = c("year", "n")
return(rbind(data, x))
}
else {return(data)}
}
# Function for removing decimal places for the sake of labels
no_dec = function(x) {sprintf("%.0f", x)}
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
library(rgdal)
library(spdplyr)
sei <- readOGR(dsn = "/Users/raghav/Desktop/DSSG19BioDiv/ShinyMap/MVSEI2014/",
layer = "MVSEI2014")
sei <- spTransform(sei, CRS("+proj=longlat +datum=WGS84"))
View(sei)
```{r}
library(rgdal)
library(spdplyr)
sei <- readOGR(dsn = "/Users/raghav/Desktop/DSSG19BioDiv/ShinyMap/MVSEI2014/",
layer = "MVSEI2014")
sei <- spTransform(sei, CRS("+proj=longlat +datum=WGS84"))
library(rgdal)
library(spdplyr)
sei <- readOGR(dsn = "/Users/raghav/Desktop/DSSG19BioDiv/ShinyMap/MVSEI2014/",
layer = "MVSEI2014")
sei <- spTransform(sei, CRS("+proj=longlat +datum=WGS84"))
library(rgdal)
library(spdplyr)
sei <- readOGR(dsn = "/Users/raghav/Desktop/DSSG19BioDiv/ShinyMap/MVSEI2014/",
layer = "MVSEI2014")
sei <- spTransform(sei, CRS("+proj=longlat +datum=WGS84"))
sei
unique(sei$SEI_PolyNb)
length(unique(sei$SEI_PolyNb))
nrow(sei)
obs_dat = read.csv("gbif_map.csv", stringsAsFactors = F)
View(obs_dat)
obs_dat = obs_dat[,c(9:10,17)]
head(obs_dat)
class(sei)
sei@data$poly_ID = 1:nrow(sei@data)
sei@data$poly_index = 1:nrow(sei@data)
poly_list = list()
# Fills "mun_list" one item at a time, using our custom function for every non-UEL polygon and the
for (ob in sei@data$poly_index) {
poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[ob,], byid = TRUE), 2, any),]
}
library(rgdal)
library(dplyr)
library(rgeos)
library(sp)
# Fills "mun_list" one item at a time, using our custom function for every non-UEL polygon and the
for (ob in sei@data$poly_index) {
poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[ob,], byid = TRUE), 2, any),]
}
coordinates(obs_dat) = c("decimalLongitude", "decimalLatitude")
proj4string(obs_dat) = CRS("+proj=longlat +datum=WGS84")
obs_dat = spTransform(obs_dat, proj4string(sei))
# Fills "mun_list" one item at a time, using our custom function for every non-UEL polygon and the
for (ob in sei@data$poly_index) {
poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[ob,], byid = TRUE), 2, any),]
}
sei@data$poly_index = NULL
sei@data$poly_ID = NULL
sei[1]
sei
sei[1,]
head(sei@data$SEI_PolyNb)
sei[100144,]
sei[which(sei@data$SEI_PolyNb==100144),]
obs_dat = read.csv("gbif_map.csv", stringsAsFactors = F)
obs_dat = head(obs_dat, 1000)
# Create a list that will hold 27 (spatial polygon) dataframes, each containing the gbif data for a given municipality/that occured within a polygon
sei@data = sei@data[order(sei@data$SEI_PolyNb),]
sei = readOGR(dsn = "Admin seiaries", layer = "Adminseiary")
obs_dat = read.csv("gbif_map.csv", stringsAsFactors = F)
obs_dat = head(obs_dat, 1000)
coordinates(obs_dat) = c("decimalLongitude", "decimalLatitude")
proj4string(obs_dat) = CRS("+proj=longlat +datum=WGS84")
obs_dat = spTransform(obs_dat, proj4string(sei))
# Fills "poly_list" one item at a time, with each element being a data frame of the data that falls within a given polygon
for (ob in sei@data$SEI_PolyNb) {
#poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[ob,], byid = TRUE), 2, any),]
poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[which(sei@data$SEI_PolyNb==ob),], byid = TRUE), 2, any),]
}
nrows(obs_dat)
nrow(obs_dat)
head(poly_list)
length(poly_list)
# Delete municipalities containing no observations
poly_list  = poly_list[sapply(poly_list, function(x) nrow(x)>0)]
length(poly_list)
# Add a column to each of these new dataframes indicating which polygon it belongs to - then extract the data frame from the spatial polygon df object, and add all of them together into a regular output dataframe.
output = NULL
# Now that points and polygons have been matched, coordinates are converted back to the system used by the map viewer and the original GBif data
poly_list2 = list()
for (ob in 1:length(poly_list)) {
poly_list2[[ob]] = spTransform(poly_list[[ob]], CRS("+proj=longlat +datum=WGS84"))
}
# Add a column to each of these new dataframes indicating which polygon it belongs to - then extract the data frame from the spatial polygon df object, and add all of them together into a regular output dataframe.
output = NULL
for (ob in 1:length(poly_list2)) {
t = cbind(poly_list2[[ob]]@data, poly_list2[[ob]]@coords)
output = rbind(output, cbind(t, rep(sei$poly_index[ob], length=nrow(t))))
}
for (ob in 1:length(poly_list2)) {
t = cbind(poly_list2[[ob]]@data, poly_list2[[ob]]@coords)
output = rbind(output, cbind(t, rep(sei$SEI_PolyNb[ob], length=nrow(t))))
}
for (ob in sei@data$SEI_PolyNb) {
t = cbind(poly_list2[[ob]]@data, poly_list2[[ob]]@coords)
output = rbind(output, cbind(t, rep(sei$SEI_PolyNb[ob], length=nrow(t))))
}
for (ob in sei@data$SEI_PolyNb) {
t = cbind(poly_list2[[ob]]@data, poly_list2[[ob]]@coords)
output = rbind(output, cbind(t, rep(sei$SEI_PolyNb[ob], length=nrow(t))))
}
poly_list2
# Fills "poly_list" one item at a time, with each element being a data frame of the data that falls within a given polygon
for (ob in sei@data$SEI_PolyNb) {
#poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[ob,], byid = TRUE), 2, any),]
poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[which(sei@data$SEI_PolyNb==ob),], byid = TRUE), 2, any),]
}
poly_list = NULL
# Create a list that will hold 27 (spatial polygon) dataframes, each containing the gbif data for a given municipality/that occured within a polygon
#sei@data = sei@data[order(sei@data$SEI_PolyNb),]
poly_list = list()
# Fills "poly_list" one item at a time, with each element being a data frame of the data that falls within a given polygon
for (ob in sei@data$SEI_PolyNb) {
#poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[ob,], byid = TRUE), 2, any),]
poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[which(sei@data$SEI_PolyNb==ob),], byid = TRUE), 2, any),]
}
poly_list_backup = poly_list
# Now that points and polygons have been matched, coordinates are converted back to the system used by the map viewer and the original GBif data
# Add a column to each of these new dataframes indicating which polygon it belongs to - then extract the data frame from the spatial polygon df object, and add all of them together into a regular output dataframe.
output = NULL
poly_list2 = list()
for (ob in poly_list) {
poly_list2[[ob]] = spTransform(poly_list[[ob]], CRS("+proj=longlat +datum=WGS84"))
t = cbind(poly_list2[[ob]]@data, poly_list2[[ob]]@coords)
output = rbind(output, cbind(t, rep(sei$SEI_PolyNb[ob], length=nrow(t))))
}
class(poly_list)
ob
length(poly_list2)
length(poly_list)
nrow(sei@data)
# Create a list that will hold 27 (spatial polygon) dataframes, each containing the gbif data for a given municipality/that occured within a polygon
sei@data = sei@data[order(sei@data$SEI_PolyNb),]
for (ob in 1:length(poly_list)) {
poly_list2[[ob]] = spTransform(poly_list[[ob]], CRS("+proj=longlat +datum=WGS84"))
t = cbind(poly_list2[[ob]]@data, poly_list2[[ob]]@coords)
output = rbind(output, cbind(t, rep(sei$SEI_PolyNb[ob], length=nrow(t))))
}
for (ob in 1:length(poly_list)) {
if (nrow(poly_list[[ob]]@coords) > 0) {
poly_list2[[ob]] = spTransform(poly_list[[ob]], CRS("+proj=longlat +datum=WGS84"))
t = cbind(poly_list2[[ob]]@data, poly_list2[[ob]]@coords)
output = rbind(output, cbind(t, rep(sei$SEI_PolyNb[ob], length=nrow(t))))
}
}
colnames(output)[ncol(output)] = "poly_index"
head(output)
nrow(output)
length(poly_list2)
# Delete municipalities containing no observations
filled_poly  = poly_list[sapply(poly_list, function(x) nrow(x)>0)]
length(filled_poly)
raaster::plot(sei)
raster::plot(sei)
nrow(obs_dat)
obs_dat = read.csv("gbif_map.csv", stringsAsFactors = F)
obs_dat$ID = 1:nrow(obs_dat)
#obs_dat = head(obs_dat, 1000)
coordinates(obs_dat) = c("decimalLongitude", "decimalLatitude")
proj4string(obs_dat) = CRS("+proj=longlat +datum=WGS84")
obs_dat = spTransform(obs_dat, proj4string(sei))
sei = sei[1:100,]
length(sei)
# Create a list that will hold 27 (spatial polygon) dataframes, each containing the gbif data for a given municipality/that occured within a polygon
sei@data = sei@data[order(sei@data$SEI_PolyNb),]
# Fills "poly_list" one item at a time, with each element being a data frame of the data that falls within a given polygon
for (ob in sei@data$SEI_PolyNb) {
#poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[ob,], byid = TRUE), 2, any),]
poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[which(sei@data$SEI_PolyNb==ob),], byid = TRUE), 2, any),]
}
poly_list = list()
# Fills "poly_list" one item at a time, with each element being a data frame of the data that falls within a given polygon
for (ob in sei@data$SEI_PolyNb) {
#poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[ob,], byid = TRUE), 2, any),]
poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[which(sei@data$SEI_PolyNb==ob),], byid = TRUE), 2, any),]
}
# Delete municipalities containing no observations
filled_poly  = poly_list[sapply(poly_list, function(x) nrow(x)>0)]
length(filled_poly)
length(poly_list)
raster::plot(sei)
runif()
runif(1)
runif(2)
sei = sei[round(runif(100)*length(sei)),]
sei <- readOGR(dsn = "/Users/raghav/Desktop/DSSG19BioDiv/ShinyMap/MVSEI2014/",
layer = "MVSEI2014")
sei = sei[round(runif(100)*length(sei)),]
sei <- readOGR(dsn = "/Users/raghav/Desktop/DSSG19BioDiv/ShinyMap/MVSEI2014/",
layer = "MVSEI2014")
sei <- spTransform(sei, CRS("+proj=longlat +datum=WGS84"))
sei = sei[round(runif(100)*length(sei)),]
raster::plot(sei)
# Create a list that will hold 27 (spatial polygon) dataframes, each containing the gbif data for a given municipality/that occured within a polygon
sei@data = sei@data[order(sei@data$SEI_PolyNb),]
poly_list = list()
# Fills "poly_list" one item at a time, with each element being a data frame of the data that falls within a given polygon
for (ob in sei@data$SEI_PolyNb) {
#poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[ob,], byid = TRUE), 2, any),]
poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[which(sei@data$SEI_PolyNb==ob),], byid = TRUE), 2, any),]
}
# Now that points and polygons have been matched, coordinates are converted back to the system used by the map viewer and the original GBif data
# Add a column to each of these new dataframes indicating which polygon it belongs to - then extract the data frame from the spatial polygon df object, and add all of them together into a regular output dataframe.
output = NULL
poly_list2 = list()
for (ob in 1:length(poly_list)) {
if (nrow(poly_list[[ob]]@coords) > 0) {
poly_list2[[ob]] = spTransform(poly_list[[ob]], CRS("+proj=longlat +datum=WGS84"))
t = cbind(poly_list2[[ob]]@data, poly_list2[[ob]]@coords)
output = rbind(output, cbind(t, rep(sei$SEI_PolyNb[ob], length=nrow(t))))
}
}
colnames(output)[ncol(output)] = "poly_index"
outside = obs_dat[which(!(obs_dat$ID %in% output$ID)),]
outside$poly_index = NA
output = rbind(output, outside)
head(output)
nrow(output)
nrow(outside)
nrow(outside) + nrow(output) - nrow(obs_dat)
nrow(outside) + nrow(output)
nrow(obs_dat)
6/279486*100
length(filled_poly)
# Delete municipalities containing no observations
filled_poly  = poly_list[sapply(poly_list, function(x) nrow(x)>0)]
length(filled_poly)
raster::plot(filled_poly)
head(poly_list2)
head(poly_list2, 20)
head(sort(table(unique(output$poly_index)), decreasing = T))
head(sort(table(unique(outside$poly_index)), decreasing = T))
class(outside)
intersect(outside$poly_index, output$poly_index)
nrow(obs_dat)
nrow(output) + nrow(outside)
output[which(output$poly_index==NA),]
output[which(outside$poly_index==NA),]
intersect(outside$ID, output$ID)
table(outside$poly_index)
outside$poly_index = NA
table(outside$poly_index)
unique(outside$poly_index)
unique(output$ID)
length(unique(output$ID))
nrow(output)
head(sort(output$ID, decreasing = T)),]
head(sort(output$ID, decreasing = T))
head(sort(table(output$ID, decreasing = T)))
head(sort(table(output$ID), decreasing = T))
sei@data[1,]
sei[1,]
sei[,1]@data
sei@data[1,]
sei[1,]@bbox
class(sei[1,]@bbox)
sei[1,]@bbox[1,1]
round(sei[1,]@bbox[1,1],10)
obs_dat[which(obs_dat$ID==26941),]
class(obs_dat)
obs_dat = read.csv("gbif_map.csv", stringsAsFactors = F)
obs_dat$ID = 1:nrow(obs_dat)
obs_dat[which(obs_dat$ID==26941),]
head(sort(table(output$poly_index), decreasing=T))
output$poly_index[which(output$ID==26941)]
raster::plot(sei[c(10955,5633),])
raster::plot(sei[which(sei@data$SEI_PolyNb %in% c(10955, 5633)),])
raster::plot(sei[which(sei@data$SEI_PolyNb %in% c(10955)),])
raster::plot(sei[which(sei@data$SEI_PolyNb %in% c(5633)),])
sei[which(sei@data$SEI_PolyNb %in% c(5633)),]@bbox
sei[which(sei@data$SEI_PolyNb %in% c(10955)),]@bbox
raster::plot(sei[which(sei@data$SEI_PolyNb %in% c(5633)),])
sei[which(sei@data$SEI_PolyNb %in% c(10955)),]
sei[which(sei@data$SEI_PolyNb %in% c(10955)),]@data
sei[which(sei@data$SEI_PolyNb %in% c(5633)),]@data
sei[which(sei@data$SEI_PolyNb == 5633),]@data
sei[which(sei@data$SEI_PolyNb == 5633),]@bbox
sei@data
View(sei@data)
length(sei@bbox)
nrow(sei)
nrow(sei@bbox)
sei@polygons
nrows(sei@polygons)
length\(sei@polygons)
length(sei@polygons)
nrow(sei@data)
class(sei)
nrow(sei)
sei[,]
sei <- readOGR(dsn = "/Users/raghav/Desktop/DSSG19BioDiv/ShinyMap/MVSEI2014/",
layer = "MVSEI2014")
sei <- spTransform(sei, CRS("+proj=longlat +datum=WGS84"))
obs_dat = read.csv("gbif_map.csv", stringsAsFactors = F)
obs_dat$ID = 1:nrow(obs_dat)
#obs_dat = head(obs_dat, 1000)
coordinates(obs_dat) = c("decimalLongitude", "decimalLatitude")
proj4string(obs_dat) = CRS("+proj=longlat +datum=WGS84")
obs_dat = spTransform(obs_dat, proj4string(sei))
sei = sei[round(seq(0,nrow(sei),nrow(sei/100))),]
seq(0,nrow(sei),nrow(sei/100))
round(seq(0,nrow(sei),nrow(sei)100))
round(seq(0,nrow(sei),nrow(sei)/100))
sei = sei[round(seq(0,nrow(sei),nrow(sei)/100)),]
# Create a list that will hold 27 (spatial polygon) dataframes, each containing the gbif data for a given municipality/that occured within a polygon
sei@data = sei@data[order(sei@data$SEI_PolyNb),]
poly_list = list()
# Fills "poly_list" one item at a time, with each element being a data frame of the data that falls within a given polygon
for (ob in sei@data$SEI_PolyNb) {
#poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[ob,], byid = TRUE), 2, any),]
poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[which(sei@data$SEI_PolyNb==ob),], byid = TRUE), 2, any),]
}
# Now that points and polygons have been matched, coordinates are converted back to the system used by the map viewer and the original GBif data
# Add a column to each of these new dataframes indicating which polygon it belongs to - then extract the data frame from the spatial polygon df object, and add all of them together into a regular output dataframe.
output = NULL
poly_list2 = list()
for (ob in 1:length(poly_list)) {
if (nrow(poly_list[[ob]]@coords) > 0) {
poly_list2[[ob]] = spTransform(poly_list[[ob]], CRS("+proj=longlat +datum=WGS84"))
t = cbind(poly_list2[[ob]]@data, poly_list2[[ob]]@coords)
output = rbind(output, cbind(t, rep(sei$SEI_PolyNb[ob], length=nrow(t))))
}
}
colnames(output)[ncol(output)] = "poly_index"
outside = obs_dat[which(!(obs_dat$ID %in% output$ID)),]
outside$poly_index = NA
nrow(output)
nrow(outside)
nrow(outside) + nrow(output)
nrow(obs_dat
nrow(obs_dat)
output = rbind(output, outside)
class(output)
class(outside)
obs_df = obs_dat
obs_dat = read.csv("gbif_map.csv", stringsAsFactors = F)
obs_dat$ID = 1:nrow(obs_dat)
outside = obs_df[which(!(obs_dat$ID %in% output$ID)),]
outside$poly_index = NA
output = rbind(output, outside)
class(outside)
obs(df)
class(obs_df)
obs_dat = read.csv("gbif_map.csv", stringsAsFactors = F)
obs_dat$ID = 1:nrow(obs_dat)
obs_df = obs_dat
outside = obs_df[which(!(obs_df$ID %in% output$ID)),]
class(outside)
output = rbind(output, outside)
ncol(output)
ncol(outside)
outside$poly_index = NA
output = rbind(output, outside)
head(output)
length(unique(output$poly_index))
unique(output$poly_index)
# Write the file
write.csv(output, "gbif_sei_poly.csv", row.names = FALSE)
colnames(output)
collapse = output[,c(15,20)]
head(collapse)
collapse = unique(collapse)
nrow(collapse)
nrow(output)
nrow(collapse[which(!is.na(collapse$poly_index)),])
source('~/Desktop/DSSG19BioDiv/ShinyMap/spatial_connectivity.R')
poly_list2[[ob]] = spTransform(poly_list[[ob]], CRS("+proj=longlat +datum=WGS84"))
library(rgdal)
library(dplyr)
library(rgeos)
library(sp)
sei <- readOGR(dsn = "/Users/raghav/Desktop/DSSG19BioDiv/ShinyMap/MVSEI2014/",
layer = "MVSEI2014")
sei <- spTransform(sei, CRS("+proj=longlat +datum=WGS84"))
obs_dat = read.csv("gbif_map.csv", stringsAsFactors = F)
obs_dat$ID = 1:nrow(obs_dat)
obs_df = obs_dat
coordinates(obs_dat) = c("decimalLongitude", "decimalLatitude")
proj4string(obs_dat) = CRS("+proj=longlat +datum=WGS84")
obs_dat = spTransform(obs_dat, proj4string(sei))
sei = sei[round(seq(0,nrow(sei),nrow(sei)/100)),]
# Create a list that will hold 27 (spatial polygon) dataframes, each containing the gbif data for a given municipality/that occured within a polygon
sei@data = sei@data[order(sei@data$SEI_PolyNb),]
poly_list = list()
# Fills "poly_list" one item at a time, with each element being a data frame of the data that falls within a given polygon
for (ob in sei@data$SEI_PolyNb) {
#poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[ob,], byid = TRUE), 2, any),]
poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[which(sei@data$SEI_PolyNb==ob),], byid = TRUE), 2, any),]
}
# Delete municipalities containing no observations
filled_poly  = poly_list[sapply(poly_list, function(x) nrow(x)>0)]
# Now that points and polygons have been matched, coordinates are converted back to the system used by the map viewer and the original GBif data
# Add a column to each of these new dataframes indicating which polygon it belongs to - then extract the data frame from the spatial polygon df object, and add all of them together into a regular output dataframe.
output = NULL
poly_list2 = list()
for (ob in 1:length(poly_list)) {
if (nrow(poly_list[[ob]]@coords) > 0) {
poly_list2[[ob]] = spTransform(poly_list[[ob]], CRS("+proj=longlat +datum=WGS84"))
t = cbind(poly_list2[[ob]]@data, poly_list2[[ob]]@coords)
output = rbind(output, cbind(t, rep(sei$SEI_PolyNb[ob], length=nrow(t))))
}
}
colnames(output)[ncol(output)] = "poly_index"
collapse = output[,c(15,20)]
collapse = unique(collapse)
outside = obs_df[which(!(obs_df$ID %in% output$ID)),]
outside$poly_index = NA
output = rbind(output, outside)
# Write the files
write.csv(output, "gbif_sei_poly.csv", row.names = FALSE)
write.csv(collapse, "gbif_sei_poly_collapsed.csv", row.names = FALSE)
raster::plot(sei)
points(poly_list2[[muns[count]]], col = "blue")
View(sei)
View(sei@data)
write.csv(sei@data, "sei_data.csv", row.names = FALSE)
>>>>>>> 0c90e93c46c873951d03aa9d9f7257204fe4f1a6
