# Note: "data" must have exactly columns "member", "year", and "n"
add_zeros1 = function(data) {
if (class(data)[1] != "data.frame") {data = as.data.frame(data)}
for (mem in unique(data$member)) {
if(nrow(data[which(data$member==mem),]) > 1) {
min = min(data$year[which(data$member == mem)], na.rm = T)
max = max(data$year[which(data$member == mem)], na.rm = T)
for (yea in (min+1):(max-1)) {
if (!(yea %in% data$year[which(data$member == mem)])) {
data = rbind(data, data.frame(member = mem, year = as.integer(yea), n = as.integer(0), stringsAsFactors = F))
}
}
}
}
return(data)
}
# For custom tag data, aggregation is done on a column-by-column (tag-by-tag) basis, necessitating a diferent function that accepts the maximum year range of the original data so that it knows to add zeros beyond the range of an individual tag (on the plus side, we don't need to worry about single-data-point cases)
add_zeros2 = function(data, min, max) {
missing_years = setdiff(min:max, data$year)
if (length(missing_years) != 0) {
x = cbind(data.frame(setdiff(min:max, data$year), 0))
colnames(x) = c("year", "n")
return(rbind(data, x))
}
else {return(data)}
}
# Function for removing decimal places for the sake of labels
no_dec = function(x) {sprintf("%.0f", x)}
runApp()
runApp()
runApp()
runApp()
runApp()
library(shiny)
library(tidyverse)
library(stringr)
# Read in the data
dfsp <- read.csv("Taxonomy_Freq.csv", stringsAsFactors = F)
df_orig <- readRDS("gbif_summary.rds")
# Record which columns in dfsp should be treated as taxonomies and which should be treated as custom tags
tax_columns = which(colnames(dfsp) %in% c("kingdom","phylum","order","class","family","genus","species"))
tax_list = colnames(dfsp)[tax_columns]
names(tax_list) = str_to_title(tax_list)
tag_columns = grep("*_binary", colnames(dfsp))
tag_list = colnames(dfsp)[tag_columns]
names(tag_list) = sub("*_binary", "", colnames(dfsp)[tag_columns])
names(tag_list) = sub("_", " ", names(tag_list))
# create a dataframe containing total num of observations for each year
yearly_obs <- group_by(df_orig, year) %>% tally() %>% drop_na()
##~~ FUNCTIONS ~~##
# Function for adding 0-value rows to aggregate tally dataframes to fill out the years between the first and last years
# Note: "data" must have exactly columns "member", "year", and "n"
add_zeros1 = function(data) {
if (class(data)[1] != "data.frame") {data = as.data.frame(data)}
for (mem in unique(data$member)) {
if(nrow(data[which(data$member==mem),]) > 1) {
min = min(data$year[which(data$member == mem)], na.rm = T)
max = max(data$year[which(data$member == mem)], na.rm = T)
for (yea in (min+1):(max-1)) {
if (!(yea %in% data$year[which(data$member == mem)])) {
data = rbind(data, data.frame(member = mem, year = as.integer(yea), n = as.integer(0), stringsAsFactors = F))
}
}
}
}
return(data)
}
# For custom tag data, aggregation is done on a column-by-column (tag-by-tag) basis, necessitating a diferent function that accepts the maximum year range of the original data so that it knows to add zeros beyond the range of an individual tag (on the plus side, we don't need to worry about single-data-point cases)
add_zeros2 = function(data, min, max) {
missing_years = setdiff(min:max, data$year)
if (length(missing_years) != 0) {
x = cbind(data.frame(setdiff(min:max, data$year), 0))
colnames(x) = c("year", "n")
return(rbind(data, x))
}
else {return(data)}
}
# Function for removing decimal places for the sake of labels
no_dec = function(x) {sprintf("%.0f", x)}
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
library(rgdal)
library(spdplyr)
sei <- readOGR(dsn = "/Users/raghav/Desktop/DSSG19BioDiv/ShinyMap/MVSEI2014/",
layer = "MVSEI2014")
sei <- spTransform(sei, CRS("+proj=longlat +datum=WGS84"))
View(sei)
```{r}
library(rgdal)
library(spdplyr)
sei <- readOGR(dsn = "/Users/raghav/Desktop/DSSG19BioDiv/ShinyMap/MVSEI2014/",
layer = "MVSEI2014")
sei <- spTransform(sei, CRS("+proj=longlat +datum=WGS84"))
library(rgdal)
library(spdplyr)
sei <- readOGR(dsn = "/Users/raghav/Desktop/DSSG19BioDiv/ShinyMap/MVSEI2014/",
layer = "MVSEI2014")
sei <- spTransform(sei, CRS("+proj=longlat +datum=WGS84"))
library(rgdal)
library(spdplyr)
sei <- readOGR(dsn = "/Users/raghav/Desktop/DSSG19BioDiv/ShinyMap/MVSEI2014/",
layer = "MVSEI2014")
sei <- spTransform(sei, CRS("+proj=longlat +datum=WGS84"))
sei
unique(sei$SEI_PolyNb)
length(unique(sei$SEI_PolyNb))
nrow(sei)
obs_dat = read.csv("gbif_map.csv", stringsAsFactors = F)
View(obs_dat)
obs_dat = obs_dat[,c(9:10,17)]
head(obs_dat)
class(sei)
sei@data$poly_ID = 1:nrow(sei@data)
sei@data$poly_index = 1:nrow(sei@data)
poly_list = list()
# Fills "mun_list" one item at a time, using our custom function for every non-UEL polygon and the
for (ob in sei@data$poly_index) {
poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[ob,], byid = TRUE), 2, any),]
}
library(rgdal)
library(dplyr)
library(rgeos)
library(sp)
# Fills "mun_list" one item at a time, using our custom function for every non-UEL polygon and the
for (ob in sei@data$poly_index) {
poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[ob,], byid = TRUE), 2, any),]
}
coordinates(obs_dat) = c("decimalLongitude", "decimalLatitude")
proj4string(obs_dat) = CRS("+proj=longlat +datum=WGS84")
obs_dat = spTransform(obs_dat, proj4string(sei))
# Fills "mun_list" one item at a time, using our custom function for every non-UEL polygon and the
for (ob in sei@data$poly_index) {
poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[ob,], byid = TRUE), 2, any),]
}
sei@data$poly_index = NULL
sei@data$poly_ID = NULL
sei[1]
sei
sei[1,]
head(sei@data$SEI_PolyNb)
sei[100144,]
sei[which(sei@data$SEI_PolyNb==100144),]
obs_dat = read.csv("gbif_map.csv", stringsAsFactors = F)
obs_dat = head(obs_dat, 1000)
# Create a list that will hold 27 (spatial polygon) dataframes, each containing the gbif data for a given municipality/that occured within a polygon
sei@data = sei@data[order(sei@data$SEI_PolyNb),]
sei = readOGR(dsn = "Admin seiaries", layer = "Adminseiary")
obs_dat = read.csv("gbif_map.csv", stringsAsFactors = F)
obs_dat = head(obs_dat, 1000)
coordinates(obs_dat) = c("decimalLongitude", "decimalLatitude")
proj4string(obs_dat) = CRS("+proj=longlat +datum=WGS84")
obs_dat = spTransform(obs_dat, proj4string(sei))
# Fills "poly_list" one item at a time, with each element being a data frame of the data that falls within a given polygon
for (ob in sei@data$SEI_PolyNb) {
#poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[ob,], byid = TRUE), 2, any),]
poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[which(sei@data$SEI_PolyNb==ob),], byid = TRUE), 2, any),]
}
nrows(obs_dat)
nrow(obs_dat)
head(poly_list)
length(poly_list)
# Delete municipalities containing no observations
poly_list  = poly_list[sapply(poly_list, function(x) nrow(x)>0)]
length(poly_list)
# Add a column to each of these new dataframes indicating which polygon it belongs to - then extract the data frame from the spatial polygon df object, and add all of them together into a regular output dataframe.
output = NULL
# Now that points and polygons have been matched, coordinates are converted back to the system used by the map viewer and the original GBif data
poly_list2 = list()
for (ob in 1:length(poly_list)) {
poly_list2[[ob]] = spTransform(poly_list[[ob]], CRS("+proj=longlat +datum=WGS84"))
}
# Add a column to each of these new dataframes indicating which polygon it belongs to - then extract the data frame from the spatial polygon df object, and add all of them together into a regular output dataframe.
output = NULL
for (ob in 1:length(poly_list2)) {
t = cbind(poly_list2[[ob]]@data, poly_list2[[ob]]@coords)
output = rbind(output, cbind(t, rep(sei$poly_index[ob], length=nrow(t))))
}
for (ob in 1:length(poly_list2)) {
t = cbind(poly_list2[[ob]]@data, poly_list2[[ob]]@coords)
output = rbind(output, cbind(t, rep(sei$SEI_PolyNb[ob], length=nrow(t))))
}
for (ob in sei@data$SEI_PolyNb) {
t = cbind(poly_list2[[ob]]@data, poly_list2[[ob]]@coords)
output = rbind(output, cbind(t, rep(sei$SEI_PolyNb[ob], length=nrow(t))))
}
for (ob in sei@data$SEI_PolyNb) {
t = cbind(poly_list2[[ob]]@data, poly_list2[[ob]]@coords)
output = rbind(output, cbind(t, rep(sei$SEI_PolyNb[ob], length=nrow(t))))
}
poly_list2
# Fills "poly_list" one item at a time, with each element being a data frame of the data that falls within a given polygon
for (ob in sei@data$SEI_PolyNb) {
#poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[ob,], byid = TRUE), 2, any),]
poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[which(sei@data$SEI_PolyNb==ob),], byid = TRUE), 2, any),]
}
poly_list = NULL
# Create a list that will hold 27 (spatial polygon) dataframes, each containing the gbif data for a given municipality/that occured within a polygon
#sei@data = sei@data[order(sei@data$SEI_PolyNb),]
poly_list = list()
# Fills "poly_list" one item at a time, with each element being a data frame of the data that falls within a given polygon
for (ob in sei@data$SEI_PolyNb) {
#poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[ob,], byid = TRUE), 2, any),]
poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[which(sei@data$SEI_PolyNb==ob),], byid = TRUE), 2, any),]
}
poly_list_backup = poly_list
# Now that points and polygons have been matched, coordinates are converted back to the system used by the map viewer and the original GBif data
# Add a column to each of these new dataframes indicating which polygon it belongs to - then extract the data frame from the spatial polygon df object, and add all of them together into a regular output dataframe.
output = NULL
poly_list2 = list()
for (ob in poly_list) {
poly_list2[[ob]] = spTransform(poly_list[[ob]], CRS("+proj=longlat +datum=WGS84"))
t = cbind(poly_list2[[ob]]@data, poly_list2[[ob]]@coords)
output = rbind(output, cbind(t, rep(sei$SEI_PolyNb[ob], length=nrow(t))))
}
class(poly_list)
ob
length(poly_list2)
length(poly_list)
nrow(sei@data)
# Create a list that will hold 27 (spatial polygon) dataframes, each containing the gbif data for a given municipality/that occured within a polygon
sei@data = sei@data[order(sei@data$SEI_PolyNb),]
for (ob in 1:length(poly_list)) {
poly_list2[[ob]] = spTransform(poly_list[[ob]], CRS("+proj=longlat +datum=WGS84"))
t = cbind(poly_list2[[ob]]@data, poly_list2[[ob]]@coords)
output = rbind(output, cbind(t, rep(sei$SEI_PolyNb[ob], length=nrow(t))))
}
for (ob in 1:length(poly_list)) {
if (nrow(poly_list[[ob]]@coords) > 0) {
poly_list2[[ob]] = spTransform(poly_list[[ob]], CRS("+proj=longlat +datum=WGS84"))
t = cbind(poly_list2[[ob]]@data, poly_list2[[ob]]@coords)
output = rbind(output, cbind(t, rep(sei$SEI_PolyNb[ob], length=nrow(t))))
}
}
colnames(output)[ncol(output)] = "poly_index"
head(output)
nrow(output)
length(poly_list2)
# Delete municipalities containing no observations
filled_poly  = poly_list[sapply(poly_list, function(x) nrow(x)>0)]
length(filled_poly)
raaster::plot(sei)
raster::plot(sei)
nrow(obs_dat)
obs_dat = read.csv("gbif_map.csv", stringsAsFactors = F)
obs_dat$ID = 1:nrow(obs_dat)
#obs_dat = head(obs_dat, 1000)
coordinates(obs_dat) = c("decimalLongitude", "decimalLatitude")
proj4string(obs_dat) = CRS("+proj=longlat +datum=WGS84")
obs_dat = spTransform(obs_dat, proj4string(sei))
sei = sei[1:100,]
length(sei)
# Create a list that will hold 27 (spatial polygon) dataframes, each containing the gbif data for a given municipality/that occured within a polygon
sei@data = sei@data[order(sei@data$SEI_PolyNb),]
# Fills "poly_list" one item at a time, with each element being a data frame of the data that falls within a given polygon
for (ob in sei@data$SEI_PolyNb) {
#poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[ob,], byid = TRUE), 2, any),]
poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[which(sei@data$SEI_PolyNb==ob),], byid = TRUE), 2, any),]
}
poly_list = list()
# Fills "poly_list" one item at a time, with each element being a data frame of the data that falls within a given polygon
for (ob in sei@data$SEI_PolyNb) {
#poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[ob,], byid = TRUE), 2, any),]
poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[which(sei@data$SEI_PolyNb==ob),], byid = TRUE), 2, any),]
}
# Delete municipalities containing no observations
filled_poly  = poly_list[sapply(poly_list, function(x) nrow(x)>0)]
length(filled_poly)
length(poly_list)
raster::plot(sei)
runif()
runif(1)
runif(2)
sei = sei[round(runif(100)*length(sei)),]
sei <- readOGR(dsn = "/Users/raghav/Desktop/DSSG19BioDiv/ShinyMap/MVSEI2014/",
layer = "MVSEI2014")
sei = sei[round(runif(100)*length(sei)),]
sei <- readOGR(dsn = "/Users/raghav/Desktop/DSSG19BioDiv/ShinyMap/MVSEI2014/",
layer = "MVSEI2014")
sei <- spTransform(sei, CRS("+proj=longlat +datum=WGS84"))
sei = sei[round(runif(100)*length(sei)),]
raster::plot(sei)
# Create a list that will hold 27 (spatial polygon) dataframes, each containing the gbif data for a given municipality/that occured within a polygon
sei@data = sei@data[order(sei@data$SEI_PolyNb),]
poly_list = list()
# Fills "poly_list" one item at a time, with each element being a data frame of the data that falls within a given polygon
for (ob in sei@data$SEI_PolyNb) {
#poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[ob,], byid = TRUE), 2, any),]
poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[which(sei@data$SEI_PolyNb==ob),], byid = TRUE), 2, any),]
}
# Now that points and polygons have been matched, coordinates are converted back to the system used by the map viewer and the original GBif data
# Add a column to each of these new dataframes indicating which polygon it belongs to - then extract the data frame from the spatial polygon df object, and add all of them together into a regular output dataframe.
output = NULL
poly_list2 = list()
for (ob in 1:length(poly_list)) {
if (nrow(poly_list[[ob]]@coords) > 0) {
poly_list2[[ob]] = spTransform(poly_list[[ob]], CRS("+proj=longlat +datum=WGS84"))
t = cbind(poly_list2[[ob]]@data, poly_list2[[ob]]@coords)
output = rbind(output, cbind(t, rep(sei$SEI_PolyNb[ob], length=nrow(t))))
}
}
colnames(output)[ncol(output)] = "poly_index"
outside = obs_dat[which(!(obs_dat$ID %in% output$ID)),]
outside$poly_index = NA
output = rbind(output, outside)
head(output)
nrow(output)
nrow(outside)
nrow(outside) + nrow(output) - nrow(obs_dat)
nrow(outside) + nrow(output)
nrow(obs_dat)
6/279486*100
length(filled_poly)
# Delete municipalities containing no observations
filled_poly  = poly_list[sapply(poly_list, function(x) nrow(x)>0)]
length(filled_poly)
raster::plot(filled_poly)
head(poly_list2)
head(poly_list2, 20)
head(sort(table(unique(output$poly_index)), decreasing = T))
head(sort(table(unique(outside$poly_index)), decreasing = T))
class(outside)
intersect(outside$poly_index, output$poly_index)
nrow(obs_dat)
nrow(output) + nrow(outside)
output[which(output$poly_index==NA),]
output[which(outside$poly_index==NA),]
intersect(outside$ID, output$ID)
table(outside$poly_index)
outside$poly_index = NA
table(outside$poly_index)
unique(outside$poly_index)
unique(output$ID)
length(unique(output$ID))
nrow(output)
head(sort(output$ID, decreasing = T)),]
head(sort(output$ID, decreasing = T))
head(sort(table(output$ID, decreasing = T)))
head(sort(table(output$ID), decreasing = T))
sei@data[1,]
sei[1,]
sei[,1]@data
sei@data[1,]
sei[1,]@bbox
class(sei[1,]@bbox)
sei[1,]@bbox[1,1]
round(sei[1,]@bbox[1,1],10)
obs_dat[which(obs_dat$ID==26941),]
class(obs_dat)
obs_dat = read.csv("gbif_map.csv", stringsAsFactors = F)
obs_dat$ID = 1:nrow(obs_dat)
obs_dat[which(obs_dat$ID==26941),]
head(sort(table(output$poly_index), decreasing=T))
output$poly_index[which(output$ID==26941)]
raster::plot(sei[c(10955,5633),])
raster::plot(sei[which(sei@data$SEI_PolyNb %in% c(10955, 5633)),])
raster::plot(sei[which(sei@data$SEI_PolyNb %in% c(10955)),])
raster::plot(sei[which(sei@data$SEI_PolyNb %in% c(5633)),])
sei[which(sei@data$SEI_PolyNb %in% c(5633)),]@bbox
sei[which(sei@data$SEI_PolyNb %in% c(10955)),]@bbox
raster::plot(sei[which(sei@data$SEI_PolyNb %in% c(5633)),])
sei[which(sei@data$SEI_PolyNb %in% c(10955)),]
sei[which(sei@data$SEI_PolyNb %in% c(10955)),]@data
sei[which(sei@data$SEI_PolyNb %in% c(5633)),]@data
sei[which(sei@data$SEI_PolyNb == 5633),]@data
sei[which(sei@data$SEI_PolyNb == 5633),]@bbox
sei@data
View(sei@data)
length(sei@bbox)
nrow(sei)
nrow(sei@bbox)
sei@polygons
nrows(sei@polygons)
length\(sei@polygons)
length(sei@polygons)
nrow(sei@data)
class(sei)
nrow(sei)
sei[,]
sei <- readOGR(dsn = "/Users/raghav/Desktop/DSSG19BioDiv/ShinyMap/MVSEI2014/",
layer = "MVSEI2014")
sei <- spTransform(sei, CRS("+proj=longlat +datum=WGS84"))
obs_dat = read.csv("gbif_map.csv", stringsAsFactors = F)
obs_dat$ID = 1:nrow(obs_dat)
#obs_dat = head(obs_dat, 1000)
coordinates(obs_dat) = c("decimalLongitude", "decimalLatitude")
proj4string(obs_dat) = CRS("+proj=longlat +datum=WGS84")
obs_dat = spTransform(obs_dat, proj4string(sei))
sei = sei[round(seq(0,nrow(sei),nrow(sei/100))),]
seq(0,nrow(sei),nrow(sei/100))
round(seq(0,nrow(sei),nrow(sei)100))
round(seq(0,nrow(sei),nrow(sei)/100))
sei = sei[round(seq(0,nrow(sei),nrow(sei)/100)),]
# Create a list that will hold 27 (spatial polygon) dataframes, each containing the gbif data for a given municipality/that occured within a polygon
sei@data = sei@data[order(sei@data$SEI_PolyNb),]
poly_list = list()
# Fills "poly_list" one item at a time, with each element being a data frame of the data that falls within a given polygon
for (ob in sei@data$SEI_PolyNb) {
#poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[ob,], byid = TRUE), 2, any),]
poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[which(sei@data$SEI_PolyNb==ob),], byid = TRUE), 2, any),]
}
# Now that points and polygons have been matched, coordinates are converted back to the system used by the map viewer and the original GBif data
# Add a column to each of these new dataframes indicating which polygon it belongs to - then extract the data frame from the spatial polygon df object, and add all of them together into a regular output dataframe.
output = NULL
poly_list2 = list()
for (ob in 1:length(poly_list)) {
if (nrow(poly_list[[ob]]@coords) > 0) {
poly_list2[[ob]] = spTransform(poly_list[[ob]], CRS("+proj=longlat +datum=WGS84"))
t = cbind(poly_list2[[ob]]@data, poly_list2[[ob]]@coords)
output = rbind(output, cbind(t, rep(sei$SEI_PolyNb[ob], length=nrow(t))))
}
}
colnames(output)[ncol(output)] = "poly_index"
outside = obs_dat[which(!(obs_dat$ID %in% output$ID)),]
outside$poly_index = NA
nrow(output)
nrow(outside)
nrow(outside) + nrow(output)
nrow(obs_dat
nrow(obs_dat)
output = rbind(output, outside)
class(output)
class(outside)
obs_df = obs_dat
obs_dat = read.csv("gbif_map.csv", stringsAsFactors = F)
obs_dat$ID = 1:nrow(obs_dat)
outside = obs_df[which(!(obs_dat$ID %in% output$ID)),]
outside$poly_index = NA
output = rbind(output, outside)
class(outside)
obs(df)
class(obs_df)
obs_dat = read.csv("gbif_map.csv", stringsAsFactors = F)
obs_dat$ID = 1:nrow(obs_dat)
obs_df = obs_dat
outside = obs_df[which(!(obs_df$ID %in% output$ID)),]
class(outside)
output = rbind(output, outside)
ncol(output)
ncol(outside)
outside$poly_index = NA
output = rbind(output, outside)
head(output)
length(unique(output$poly_index))
unique(output$poly_index)
# Write the file
write.csv(output, "gbif_sei_poly.csv", row.names = FALSE)
colnames(output)
collapse = output[,c(15,20)]
head(collapse)
collapse = unique(collapse)
nrow(collapse)
nrow(output)
nrow(collapse[which(!is.na(collapse$poly_index)),])
source('~/Desktop/DSSG19BioDiv/ShinyMap/spatial_connectivity.R')
poly_list2[[ob]] = spTransform(poly_list[[ob]], CRS("+proj=longlat +datum=WGS84"))
library(rgdal)
library(dplyr)
library(rgeos)
library(sp)
sei <- readOGR(dsn = "/Users/raghav/Desktop/DSSG19BioDiv/ShinyMap/MVSEI2014/",
layer = "MVSEI2014")
sei <- spTransform(sei, CRS("+proj=longlat +datum=WGS84"))
obs_dat = read.csv("gbif_map.csv", stringsAsFactors = F)
obs_dat$ID = 1:nrow(obs_dat)
obs_df = obs_dat
coordinates(obs_dat) = c("decimalLongitude", "decimalLatitude")
proj4string(obs_dat) = CRS("+proj=longlat +datum=WGS84")
obs_dat = spTransform(obs_dat, proj4string(sei))
sei = sei[round(seq(0,nrow(sei),nrow(sei)/100)),]
# Create a list that will hold 27 (spatial polygon) dataframes, each containing the gbif data for a given municipality/that occured within a polygon
sei@data = sei@data[order(sei@data$SEI_PolyNb),]
poly_list = list()
# Fills "poly_list" one item at a time, with each element being a data frame of the data that falls within a given polygon
for (ob in sei@data$SEI_PolyNb) {
#poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[ob,], byid = TRUE), 2, any),]
poly_list[[ob]] = obs_dat[apply(gIntersects(obs_dat, sei[which(sei@data$SEI_PolyNb==ob),], byid = TRUE), 2, any),]
}
# Delete municipalities containing no observations
filled_poly  = poly_list[sapply(poly_list, function(x) nrow(x)>0)]
# Now that points and polygons have been matched, coordinates are converted back to the system used by the map viewer and the original GBif data
# Add a column to each of these new dataframes indicating which polygon it belongs to - then extract the data frame from the spatial polygon df object, and add all of them together into a regular output dataframe.
output = NULL
poly_list2 = list()
for (ob in 1:length(poly_list)) {
if (nrow(poly_list[[ob]]@coords) > 0) {
poly_list2[[ob]] = spTransform(poly_list[[ob]], CRS("+proj=longlat +datum=WGS84"))
t = cbind(poly_list2[[ob]]@data, poly_list2[[ob]]@coords)
output = rbind(output, cbind(t, rep(sei$SEI_PolyNb[ob], length=nrow(t))))
}
}
colnames(output)[ncol(output)] = "poly_index"
collapse = output[,c(15,20)]
collapse = unique(collapse)
outside = obs_df[which(!(obs_df$ID %in% output$ID)),]
outside$poly_index = NA
output = rbind(output, outside)
# Write the files
write.csv(output, "gbif_sei_poly.csv", row.names = FALSE)
write.csv(collapse, "gbif_sei_poly_collapsed.csv", row.names = FALSE)
raster::plot(sei)
points(poly_list2[[muns[count]]], col = "blue")
View(sei)
View(sei@data)
write.csv(sei@data, "sei_data.csv", row.names = FALSE)
